{"id":"103757","title":"Emotionally-based Tagging of Multimedia Content","abstractText":"'As the amount of digital multimedia content increases while the cost of storing digital multimedia decreases, it becomes very difficult to locate a specific content. To benefit from this wealth of multimedia data, numerous methods for automatic multimedia indexing and retrieval have been proposed. However, these methods usually depend on having large amounts of annotated data to learn from. The main idea behind Implicit Human-Centred Tagging (IHCT), which is the main topic of this proposal, is automatic extraction of tags based on user's behaviour or response while watching multimedia content. The underlying assumption of IHCT is that observable nonverbal behaviour while interacting with multimedia content (e.g. affective facial expressions, head nods and shakes, laughter, pupil dilation, etc.) provide information useful for improving the tags associated with the data. As such responses are generated naturally and spontaneously, no specific effort is required from the users, and this is why the resulting tagging is said to be �implicit� (rather than explicit). The main aim of the EmoTag project is to develop and evaluate an affect-sensitive implicit tagging system based on continuous assessment of affective responses of users. Specifically, the project aims to answer whether users' behaviour can be used to inform the system about the possible tags that could be used to describe the multimedia content and  how automatic tools for multimedia content tagging could be redesigned to benefit from the implicit tags. The project will also investigates whether retrieval/recommendation systems based on implicit tags will perform comparably to those based on explicit tags. The other aspect of the  proposed study deals with automatic continuous dimensional affect recognition from multiple modalities and proposes a number of novel machine learning techniques that promise to\nsolve this complex problem characterized by high-dimensional feature space and low amount of sample data.'","grantUrl":"","grantId":"300318","fundValue":"200371.8","fundStart":"2012-05-01","fundEnd":"2014-04-30","dataset":"fp7"}