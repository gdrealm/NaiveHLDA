{"id":"C6F35F14-A777-41AB-9F7C-8246570848B8","title":"Performance based expressive virtual characters","abstractText":"Creating believable, expressive interactive characters is one of the great, and largely unsolved, technical challenges of interactive media. Human-like characters appear throughout interactive media, virtual worlds and games and are vital to the social and narrative aspects of these media, but they rarely have the psychological depth of expression found in other media. This proposal is for the development of research into a new approach to creating interactive characters which identifies the central problem of current methods as being the fact that creating the interactive behaviour, or Artificial Intelligence (AI), of a character is still primarily a programming task, and therefore in the hands of people with a technical rather than an artistic training. Our hypothesis is that the actors' artistic understanding of human behaviour will bring an individuality, subtlety and nuance to the character that it would be difficult to create in hand authored models. This will help interactive media represent more nuanced social interaction, thus broadening their range of application. The proposed research will use information from an actor's performance to determine the parameters of a character's behaviour software. We will use Motion Capture to record an actor interacting with another person. The recorded data will be used as input to a machine learning algorithm that will infer the parameters of a behavioural control model for the character. This model will then be used to control a real time animated character in interaction with a person. The interaction will be a full body interaction involving motion tracking of posture and/or gestures, and voice input.In entertainment this method will enable more social genres and help improve the current limited demographic. It will also enable a number of new applications in education, rehabilitation, media and marketing. Putting actors in charge of creating character AI will also make production pipelines more efficient be requiring less input from programmers This project is timely in that it brings together a number of active and developing research fields including expressive virtual characters, motion capture based animation and machine learning. It has the potential to transform current research in expressive virtual characters and present new research problems for machine learning and motion capture based animation. It is novel in that it proposes a fundamentally new approach to creating interactive characters and it combines disciplines such as animation, statistical machine learning, performance, affective computing, human computer interaction and psychology. The use of both machine learning and performance for virtual characters is particularly novel.","grantUrl":"http://gtr.rcuk.ac.uk/projects?ref=EP/H02977X/1","grantId":"EP/H02977X/1","fundValue":"100636","fundStart":"2010-09-21","fundEnd":"2012-03-01","funder":"EPSRC","impactText":"  The findings of the research project provide the foundation for further research into interactive machine learning for virtual characters and has lead to several possible new research collabroations as well as high profile public engagement.  ","person":"Marco  Gillies","coPersons":[],"organisation":"Goldsmiths College","findingsText":" This project has developed a new approach to designing video game characters that can respond to our body movements and body language. Rather than trying to program explicit rules for behavior, which would make it hard to capture the subtleties of body language, our software allows people to design movements directly by moving and interacting. \n\n\n\nWe have developed two game environments in which people can customize characters' behaviour based on their own movements. \n\n\n\nThe first is a 3D version of the classic video game Pong, in which players can control the paddles and their avatars with their own movements. They were able to customize their avatars responses to winning or loosing points based on their own movements. In our user tests, our participants found that performing the actions themselves helped them understand the game better and made it easier for them to design the avatar's responses.\n\n\n\nThe second examples is a 3D character that responds to a players body movements via the Microsoft Kinect motion tracking system. The character's responses are designed based on motion capture of a real interaction between people. Two people can play the roles of the video game character and the player, showing how the character should respond by acting out the movements themselves. This allows them to design movements in a natural way, by moving, rather than having to think about mathematical rules. The motion of both participants are recorded and synchronized. This data is then used as input to a machine learning algorithm which learns an algorithm for automatically controlling a video game character so that is responds in the same way as the people designing it. \n\n\n\nThis style of design is particularly well suited to actors and performers who have a deep understanding of movement and body language. We did an in depth case study with physical theatre performer Emanuele Nargi, who used our software to design an interactive character based on his interactions with a number of members of the public. The results of this research are aimed at the video games industry and other interactive media. The release, during the project, of the Microsoft Kinect consumer-level motion tracking system, has opened up the potential for games and other interactive experiences that respond directly to players' movements and body language. The project has been able to tap into this potential and develop demonstrators that show how both end users and professionals can use motion tracking to develop interactive Creative Economy","dataset":"gtr"}