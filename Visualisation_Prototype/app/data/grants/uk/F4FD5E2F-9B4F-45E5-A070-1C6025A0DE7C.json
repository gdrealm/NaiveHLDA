{"id":"F4FD5E2F-9B4F-45E5-A070-1C6025A0DE7C","title":"Perceptual constancy in real-room listening by humans and machines","abstractText":"Our perceptual 'systems' allow us reliably to judge properties of things in the real world under diverse conditions, as they exhibit 'constancy'. For example, a white surface in dim light can be distinguished from a black surface in bright light, even though the luminance of the two surfaces might be the same. Although constancy is clearly vital for survival, and has been extensively studied in vision, it has not been investigated in hearing very much. This lack of knowledge probably accounts for the poor performance of the current generation of artificial listening devices, which are becoming increasingly important in hearing aids, as well as in other applications of automated speech recognition. We aim to measure the different listening conditions effected by real rooms and then to investigate constancy in hearing with perceptual experiments. This information will then be incorporated into prototype artificial-listening devices, which will be tested for their effectiveness in dealing with the real world conditions that human hearing seems to cope with so exquisitely.","grantUrl":"http://gtr.rcuk.ac.uk/projects?ref=EP/G009805/1","grantId":"EP/G009805/1","fundValue":"128310","fundStart":"2008-10-20","fundEnd":"2012-04-19","funder":"EPSRC","impactText":"  To date, all impact has been scientific. We are currently planning an EPSRC proposal on hearing aid technology which will provide a route to exploit some of the research on perceptual compensation within the healthcare field. Healthcare Societal","person":"Guy  Brown","coPersons":[],"organisation":"University of Sheffield","findingsText":" This project investigated perceptual compensation for the effects of room reverberation on speech recognition, using experiments with both human listeners and machine systems (auditory models and automatic speech recognition (ASR) systems). Our key findings were:\n\n\n\nPerceptual data on compensation for the effects of reverberation in a 'sir-stir' identification task was closely matched by a computer model of auditory efferent processing. In the computer model, the peripheral auditory response was suppressed by efferent activity, which in turn was determined by the dynamic range of the preceding auditory nerve response. The model therefore suggests that perceptual compensation for the effects of reverberation is underlain, at least in part, by auditory mechanisms responsible for the monitoring and control of dynamic range.\n\n\n\nThe computer model was able to explain Watkins' finding that perceptual compensation in the 'sir-stir' task was not affected by time-reversing the speech preceding the test word, but that compensation was abolished when the room impulse response was reversed. Time reversal of the speech preceding the test word (the 'context') did not substantially affect the dynamic range of the auditory nerve response, so our model made the same predictions in the forward-speech and reverse-speech conditions. For these experimental stimuli, however, reversing the room impulse response increased the dynamic range of the context prior to the test word. This caused a reduction in the efferent response and removed the compensation effect.\n\n\n\nWe have demonstrated that perceptual compensation for reverberation was apparent for human listeners in a consonant identification task using naturalistic speech. Test words included a wider range of consonants (/t/, /k/, /p/), six vowels, and were spoken by both male and female voices in realistically variable speech utterances. Compensation was apparent in the pattern of confusions made by listeners, as quantified in terms of the relative information transmitted. When more reverberation was added to the test word only, listeners confused many consonants. These confusions were largely resolved when similar reverberation was added to the speech preceding the test word.\n\n\n\nUsing this (/t/, /k/, /p/) consonant identification task, we confirmed Watkins' finding that perceptual compensation is abolished when the room impulse response is time-reversed. We found also that the reverberation tail of the test word contributes to perceptual compensation under certain conditions. Compensation was reduced when the reverberation tail of the test word was partially removed by gating, but this effect was only apparent when the context preceding the test word did not itself promote compensation. Further, we investigated the time course of perceptual compensation using reverberant contexts of gradually increasing duration and found that compensation increased with increasing context duration up to 500 ms, the maximum time course considered. \n\n\n\nA reverberation-robust ASR system was developed in which different statistical models of speech were selected according to the dynamic range of the speech signal. The model showed a close match to the confusion patterns made by human listeners in our perceptual data.\n\n\n\nFinally, principles derived from our perceptual experiments (within-band processing and the role of reverberation tails) were implemented in a 'missing feature' ASR system. In this approach, a number of acoustic cues (including those derived from a model of binaural processing) were employed to identify time-frequency regions of speech that were relatively uncorrupted by reverberation. The 'clean' regions were used directly for recognition, whereas the true values of the corrupted regions were imputed (reconstructed) from statistical models of speech. The ASR system gave a good performance on the CHiME challenge, an evaluation corpus in which the speech is corrupted by reverberation and environmental noise. Hearing impaired listeners have particular problems understanding speech in reverberant environments. Our computer models and psychophysical data could be used to improve the performance of hearing aids and cochlear implants in reverberant conditions. The results of this research will be of interest to researchers developing reverberation-robust automatic speech recognition systems, which wish to include human-like processing in their algorithms. Our computer model makes predictions that can be Healthcare","dataset":"gtr"}