{"id":"C7E030EC-A33B-4674-B637-4F999EB32CA2","title":"Robust Syllable Recognition in the Acousic-Waveform Domain","abstractText":"This proposal is concerned with robust classification/recognition of speech units (phonemes and consonant-vowel syllables) in the domain of acoustic waveforms. The motivation for this research comes from the idea that speech units should be much better separated in the high-dimensional spaces formed by acoustic waveforms than in the smaller representation spaces which are used in state-of-the-art speech recognition systems and which involve significant compression and dimension reduction. Hence, recognition/classification in the acoustic waveform domain should exhibit a higher level of robustness to additive noise than classification in low-dimensional feature spaces.In the first phase of the project we will investigate classification of speech units in the acoustic waveform domain under severe noise conditions, around 0dB signal-to-noise ratio and below, while in the second phase we will study techniques which would make classification robust also to linear filtering. The particular tasks that will be tackled in the first phase can be summarized as follows:1. Study the detailed structure of the sets of acoustic waveforms of individual speech units; in particular their intrinsic dimensions, and the existence of possible nonlinear surfaces on which the data are concentrated.2. Guided by the findings from item 1 above, estimate statistical models of the distribution of speech units in the acoustic waveform domain. We will then design and systematically assess so-called generative classifiers, whose defining property is that they are based on such statistical models.3. Investigate classification of speech units in the acoustic waveform domain using discriminative classification techniques (artificial neural networks, support vector machines, and relevance vector machines). These can be a useful alternative to generative techniques because they focus directly on the classification problem without building explicit models of waveform distributions for each speech unit.4. Construct classifiers by grouping speech units hierarchically. Top-level classifiers will be constructed to distinguish between a small of groups of similar speech units, followed by classifiers separating groups into subgroups and so on. Different methods for defining subgroups will be explored, including confusion matrices of the classifiers from item 3, appropriate distance measures between the statistical models obtained in item 2, and possibly perceptual experiments.A potential argument against our approach is that classification in the acoustic waveform domain will break down in the presence of linear filtering. However, this can be avoided by considering narrow-band signals: for these, the effect of linear filtering is approximately equivalent to amplitude scaling and time delay. In the second phase of the project, we will therefore consider speech classification using narrow-band components of acoustic waveforms. For classification of signals in individual sub-bands, the techniques investigated in the first phase of the project will be considered. A new issue is then how to combine the results of sub-band classifiers to minimize the overall classification error. Here recently developed machine learning techniques will be used, as specified in the case for support.As explained, individual sub-band classifiers should be robust to linear filtering because the latter does not significantly alter the shape of narrow-band signals. On the other hand, the dimension of the spaces of sub-band waveforms will be still high enough to facilitate classification robust to additive noise. Hence, the overall scheme is expected to be robust to both additive noise and linear fitering.","grantUrl":"http://gtr.rcuk.ac.uk/projects?ref=EP/D053005/1","grantId":"EP/D053005/1","fundValue":"207533","fundStart":"2006-10-01","fundEnd":"2010-03-31","funder":"EPSRC","impactText":"  The project was a proof-of-concept study, exploring a paradigm shift concept in automatic speech recognition. As such, findings of the project are still of fundamental scientific nature, but have gained us partnership with researchers from the University of California, Berkeley, and SRI (Stanford Research Institute) International for further exploration of the developed concepts to the point where they could be successfully deployed in practical automatic speech recognition systems.  ","person":"Zoran  Cvetkovic","coPersons":["Peter  Sollich"],"organisation":"King's College London","findingsText":" The project has demonstrated a significant potential of solving the long standing issue of the lack of robustness of automatic speech recognition (ASR) systems by posing the problem in high-dimensional spaces of acoustic waveforms of speech, possibly transformed by some linear orthogonal transforms. Further, for that purpose generative and discriminative (support vector machine) models are developed on this project. The findings of this research are relevant for products and systems in defence, healthcare, and various other telecommunications and information systems, where speech is or can be used as mode of\n\nhuman-computer interaction. Findings of the project open up a new direction in the area of ASR as well as issues of learning in high dimensions and kernel methods, which are area of intense activity is several academic communities, including statistics, computer science, signal processing Aerospace, Defence and Marine,Digital/Communication/Information Technologies (including Software),Electronics,Healthcare,Leisure Activities, including Sports, Recreation and Tourism,Security and Diplomacy","dataset":"gtr"}