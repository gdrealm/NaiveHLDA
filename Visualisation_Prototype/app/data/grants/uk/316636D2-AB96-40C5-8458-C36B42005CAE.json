{"id":"316636D2-AB96-40C5-8458-C36B42005CAE","title":"Cerebral processing of affective nonverbal vocalizations: a combined fMRI and MEG study.","abstractText":"Recognizing and interpreting emotions in other persons is crucial for social interactions. In particular, people of all cultures are able to recognize emotions in vocalizations without speech such as laughs, cries or screams of fear. But how our brain analyses emotion in voices remains poorly understood, compared to how we perceive emotion in faces, for example. In this project we combine a range of advanced techniques to precisely map the brain network involved in recognizing emotions from the voice and determine its exact time-course. We will first use recent morphing technology to manipulate a database of affective voices in order to generate new vocalizations with more or less intense, possibly ambiguous expressions (e.g., pleasure mixed with fear). We will then measure a number of parameters in the vocalizations thus generated. A large group of listeners will be asked to rate each vocalization on what emotion they think is expressed, on how intense and on how positive/negative they think it is. We will also precisely measure important physical properties of the sounds such as their intensity and pitch. In parallel we will use state-of-the art, complementary brain imaging techniques (functional magnetic resonance imaging and magneto-encephalography) to measure brain activity in a smaller number of participants while they listen to the affective vocalizations and perform simple tasks-a Male/Female gender categorisation task, and a Fear/Anger/Pleasure emotion categorization task. The combination of these two brain imaging techniques will allow measurements of cerebral activity with high time (millisecond) and space (millimetre) accuracy. Analysis of this high-resolution, high density dataset will use the most recent algorithms to address three important, unresolved questions. First we want to differentiate the part of the brain that reacts to the acoustics in the sounds - a vocalization of pleasure sounds different from an angry shout-from that part of the brain that reflects genuine affective value-these two vocalizations express different affective states in the speaker. This important distinction has generally not been adequately addressed in past studies. Second, we want to better understand, in those parts of the brain genuinely related to emotional processing, exactly to which parameter they react: To the emotional category, e.g., a response to fear but not to pleasure? To the negative/positive dimension, e.g., a response to all threatening sounds but not happy or joyful sounds? Or to the task being performed by the subject, e.g., a response during an emotional task but not a gender task? Third, we want to better understand the time-course of processing of affective information at different nodes of the network of brain areas involved in processing these vocalizations. If what has been observed with facial expressions of emotion also applies to voices, then we should observe a very fast, probably unconscious reaction to affective vocalizations, in a 'fast route' that bypasses detailed analysis for the sake of a fast reaction. Advanced algorithms will allow us to determine the precise time course of neuronal activity in different parts of the brain network and understand how different emotional parameters affect the speed of brain reaction. Overall, the results of this project will allow us to understand how the brain processes a socially central dimension of voices-the emotion they carry-and what are the key parameters. They will contribute to the advancement of knowledge, but also in the longer term to a better understanding of impairments of emotion processing in pathologies such as autism or schizophrenia. They also have high potential importance for the growing industry of automated voice processing, as engineers need to know not only how to best automatically recognize emotions in people but also how to best generate realistic emotions in artificial voices.","grantUrl":"http://gtr.rcuk.ac.uk/projects?ref=BB/J003654/1","grantId":"BB/J003654/1","fundValue":"263101","fundStart":"2012-09-21","fundEnd":"2014-09-20","funder":"BBSRC","impactText":"","person":"Pascal  Belin","coPersons":["Joachim  Gross"],"organisation":"University of Glasgow","findingsText":"","dataset":"gtr"}