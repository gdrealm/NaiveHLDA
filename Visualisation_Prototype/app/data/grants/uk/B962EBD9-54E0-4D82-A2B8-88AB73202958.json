{"id":"B962EBD9-54E0-4D82-A2B8-88AB73202958","title":"Structured Sparsity Methods in Machine Learning an Convex Optimisation","abstractText":"Over the past ten years theoretical developments in machine learning (ML) have had a significant impact in statistics, applied mathematics and other fields of scientific research. In particular, fruitful interactions between ML and numerical optimisation have emerged that are expected to lead to theoretical and algorithmic breakthroughs with the potential to render ML methodologies significantly more applicable to many problems of practical importance. The proposed project aims to make significant UK contributions at a crucial juncture in this emerging interdisciplinary field that has so far been dominated by the US and France. Many ML techniques can be cast as problems of minimising an objective function over a large set of parameters. Examples include support vector machines as well as more recent techniques for semi-supervised learning and multi-task learning. Often the objective function is convex. Consequently, ideas from convex optimisation are becoming increasingly important in the design, implementation and analysis of learning algorithms. Up to now, however, ML has almost exclusively resorted to off the shelf methods for convex optimisation, without substantially exploiting the rich theory which lies behind this field. A thesis of this proposal is that there is a need for a deeper interplay between ML and numerical optimisation. Ultimately, bridging the two communities will facilitate communication and the power of core optimisation will be more easily brought to bear in ML and lead to new frontiers in optimisation. An area in which the interplay between ML and optimisation has a particularly important role to play is in the use of sparsity inducing optimisation problems. A rationale that drives the use of sparsity-inducing models is the observation that when the number of model parameters is much larger than the number of observations, a sparse choice of parameters is strongly desirable for fast and accurate learning. Building on this success, we believe that the time is now right for the development of a new line of algorithms for matrix learning problems under structured sparsity constraints. This means that many of the components of the parameter matrix or a decomposition thereof are zero in locations that are related via some rule (e.g the matrix may be constrained to have many zero rows, many zero eigenvalues, to have sparse eigenvectors, etc.).Perhaps the most well-know examples in which structured sparsity has proven beneficial are in collaborative filtering, where the objective function is chosen to favour low rank matrices, and in multi-task learning where the objective function is chosen to favour few common relevant variables across different regression equations. These types of optimisation problems have only recently started to be addressed in ML and optimisation, and several fundamental problems remain open, most importantly the study of efficient algorithms which exploit the underlying sparsity assumptions and a statistical learning analysis of the methods.Our proposal is multidisciplinary and involves substantial exchange of ideas between Computer Science (Machine Learning) and Mathematics (Numerical Optimisation), with three main goals. Firstly, we aim to develop novel and efficient algorithms for learning large structured matrices; fast convergence of the algorithms should be guaranteed when applied to problem data that have a sparse solution. Secondly, in the cases where the assumed sparsity structure leads to NP-hard problems and the first goal is unachievable (this is often the case under low-rank assumptions), we aim to identify tractable convex relaxations and understand their impact on sparsity. Thirdly, we aim for models and algorithms that have a more natural interpretation than generic solvers (e.g., a minimax statistical justification), which should make it more likely that practitioners will embrace the new methodology.","grantUrl":"http://gtr.rcuk.ac.uk/projects?ref=EP/H02686X/1","grantId":"EP/H02686X/1","fundValue":"153264","fundStart":"2010-09-22","fundEnd":"2014-03-21","funder":"EPSRC","impactText":"  Radius Health Ltd is taking off as a successful UK startup company, building microemitter arrays that will use an imaging methodology that we pioneered under the project. \n\nThe Operational Risk Management Team at Banco Santander are looking into the use of our robust risk aggregation model and algorithm under their library of methodologies used to compute the capital charge on their balance sheet. Financial Services, and Management Consultancy,Healthcare Societal,Economic,Policy & public services","person":"Raphael  Hauser","coPersons":[],"organisation":"University of Oxford","findingsText":" Many optimisation algorithms in machine learning contain approximate singular value decompositions as a bottleneck computation. One of our focusses is therefore the development of highly scalable, loosely coupled, communication poor parallel algorithms for computing an approximate leading part singular value decomposition of large scale matrices. We identified a class of such algorithms and analysed their convergence analysis. \n\n\n\nAnother focus is on applying machine learning techniques to the design of statistical significance tests of optimal sequence alignments by taking the empirical distribution of aligned letter pairs into account. So far we made important inroads into this question by proving that the asymptotic empirical distribution is unique for almost all scoring functions, and by developing an efficient Monte Carlo technique for determining the order of fluctuation. This analysis exhibits a deep connection between convex analysis and large deviations theory.\n\nIn a third focus point, we investigated optimisation problems over measure spaces in which the optimal measure has a sparse representation as a sum of simple functions. We identified a similar problem structure in a medical imaging problem (3D X-ray imaging with a microemitter array) and in a risk management context (aggregation of risks under marginal constraints). We developed specialised models and algorithms to be able to solve such problems in high dimensions. This research continues. \n\nIn a fourth focus, we are investigating a game theoretic approach to explaining the energy mix and prices in deregulated electricity markets. To do this, a thorough understanding of the market mechanisms and physical properties of power plants and the grid are needed to model the structure of the optimisation problems solved by the different market participants. We successfully set up a model that is able to explain price spikes and the prevailing power mix under consideration of startup costs, trading costs and risk. Further refinements of the model will allow us to study the impact on price stability of adding renewables to the energy mix and could be used by policy makers. Data mining in big data applications (large-scale principle component analysis on dense matrices).\n\nStep change in medical imaging (highly portable and affordable X-ray systems based on microemitter arrays).\n\nImproved statistical tests for the homology of finite sequences (applied in genetics, forensics, natural language processing).\n\nImproved risk management in the banking and insurance sector (through robust bounds on the capital at risk through their balance sheet). Aerospace, Defence and Marine,Creative Economy,Digital/Communication/Information Technologies (including Software),Energy,Environment,Financial Services, and Management Consultancy,Healthcare,Transport","dataset":"gtr"}