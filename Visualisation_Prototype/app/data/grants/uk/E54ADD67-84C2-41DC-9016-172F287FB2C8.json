{"id":"E54ADD67-84C2-41DC-9016-172F287FB2C8","title":"A Tongue Movement Command and Control System Based on Aural Flow Monitoring","abstractText":"Although there is a well-recognized need in society for effective tools which will enable the physically impaired to be more independent and productive, existing technology still fails to meet many of their needs. In particular, nearly all mechanisms designed for human-control of peripheral devices require users to generate the input signal through bodily movements, most often with their hands, arms, legs, or feet. Such devices clearly exclude individuals with limited appendage control. Spinal cord injuries, repetitive strain injuries, severe arthritis, loss of motion due to stroke, and central nervous system (CNS) disorders all represent examples of these impairments. Past work has attempted to address this need through recognition of the potential of the oral cavity (victims of stroke, spinal damage, and arthritis can often move their tongue and mouth) for control input. Developed mechanisms include: track balls, small joysticks, and retainers inserted in the mouth to be manipulated by the tongue, or sip and puff tubes which respond to concentrated exhaling and inhaling. Despite their numerous successful implementations, these devices can be difficult to operate, problematic if they fall from or irritate the mouth, may impair verbal communication, and present hygiene issues since they must be inserted within the mouth.The objective of this programme is to surmount these issues through the development of a unique tongue-movement communication and control strategy in a stand-alone device that can be calibrated for patient-use with all manner of common household devices and tailored to control assistive mechanisms. The strategy is based on detecting specific tongue motions by monitoring air pressure in the human outer ear, and subsequently providing control instructions corresponding to that tongue movement for peripheral control. Our research has shown various movements within the oral cavity create unique, traceable pressure changes in the human ear, which can be measured with a simple sensor (e.g. a microphone) and analyzed to produce commands, that can in turn be used to control mechanical devices or other peripherals. Results from this programme will enable patients with quadriplegia, arthritis, limited movement due to stroke, or other conditions causing limited or painful hand/arm movement to interface with their environment and control all manner of equipment for increased independence and quality of life.","grantUrl":"http://gtr.rcuk.ac.uk/projects?ref=EP/F01869X/1","grantId":"EP/F01869X/1","fundValue":"269165","fundStart":"2008-04-01","fundEnd":"2011-09-30","funder":"EPSRC","impactText":"  Findings from this program created the first ever tongue based machine interface system that required no oral insertion. It led to the awards: &quot;A Teleoperative Sensory-Motor Control System&quot; (Imperial College-NUS Singapore), &quot;Integrated motion, muscle, and neural activity monitoring&quot; (EPSRC kick-start internal award), and &quot;Multimodal Hands'-Free Interfaces for Robotic Teleoperation&quot; (US Army to Think-A-Move (TAM), Ltd for commercialization; reference: W56HZV-05-C-103) worth ?800,000 collectively. It spawned new products in human-robot interface being currently adopted by the US military. Industrial adoption also resulted in the first tongue controlled wheelchair with no oral obstruction. \n\nThe work was also was also highlighted in a keynote video for the 'New Technology Foundation Award' recognizing highest innovation at the 20 year anniversary IEEE IROS conference. The Engineer (2008) and New Scientist (2010) featured the research; a New Scientist video showing the first tongue controlled prosthetic hand received 6,000+ views its first week. Aerospace, Defence and Marine,Healthcare Societal,Economic","person":"Ravi  Vaidyanathan","coPersons":["Maria  Stokes","Mark Edward Lutman"],"organisation":"University of Bristol","findingsText":" The goal of the project was to demonstrate a new concept for human-machine interface in assistive device control. While extensive research has been performed in human-machine interface (HMI) for command and control, to date, nearly all interfaces are limited in their utility outside controlled environments due to the need for operator motion, lack of portability, cumbersome interface with other approaches, and equipment expense. In this program, we present the first system we are aware of that addresses all these issues. The system is capable of tracking tongue movement to indicate operator desire by monitoring airflow in the ear canal, thus no external operator movement is required. The system is unobtrusive, trivial to don, and leaves the patient free to execute any other activity while wearing/using the system. The only sensor necessary is a simple microphone and earpiece housing which is small enough and comfortable enough to be worn in the ear indefinitely. To our knowledge, our research team is the only group that has investigated the aural cavity as a monitoring venue for machine interface, and has proposed the only system whereby tongue movement may be tracked without insertion of any device in the oral cavity. We have observed tongue movement to be faster, quieter, and (in most cases) more intuitive to the user for direct device motion control compared to speech.\n\nWe have further investigated the synergy of the system with existing neural interfaces, with specific focus on brain implants and surface brain electrodes which could be combined with the tongue motion system to form a in comprehensive interface, and expanded our research results to address pattern classification of movement with respect to a range of biosignals. In summary, new contributions in this program include:\n\n\n\n1. A new sensor for monitoring airflow in the aural cavity\n\n2. Implementation of signal capture and recognition algorithms to accurately identify and classify tongue movement through monitoring of airflow in the aural cavity \n\n3. A strategy for detecting and classifying simple and compound tongue movements based on airflow in the aural cavity for robust hands-free HMI\n\n4. New multi-channel pattern classification algorithms to accurately identify physiological signals and correlate them with user intent; the implementation of these algorithms on both tongue movement and neural signals\n\n5. A novel signal capture and segmentation system to identify physiological signals and separate them from other interfering signals in the body; the implementation of these signals for real-time extraction, segmentation, and disturbance rejection of tongue movement ear pressure signals and off-line classification of neural signals\n\n6. The first ever published real-time results of a tongue human-machine interface with no insertion of any device in the oral cavity. The system has been demonstrated for real-time control of a robotic (prosthetic) assist arm, and in simulation on a power assist wheelchair. \n\n7. The development of a software system including a graphic user interface (GUI) that a user may use to calibrate the system to recognize their unique tongue signals\n\nResearch output and impact for the programme has been prolific. This research has produced 8 refereed journal publications, with another 2 currently in preparation, and more than 20 refereed conference publications. Public impact for this work includes major media coverage in magazines including New Scientist, the Association of Computing Machinery (ACM), and The Engineer, and being featured on a 2010 BBC West television episode of &amp;amp;quot;Inside Out&amp;amp;quot;. Videos associated with BBC Inside Out and New Scientist featured the first ever tongue controlled prosthetic hand. Commercial applications include prosthetic limbs and rehabilitation/assist equipment, including interfaces for power wheelchair control. Although the device is not universally applicable for any situation (e.g. when force feedback is required) we believe it represents a very significant contribution to assistive technology. We believe this work will lay the foundation for a new generation of hands-free human machine interface systems. \n\n\n\nAt this time, the company Think-A-Move has been given the results and is consolidating them for use with a new tongue-controlled wheelchair. The prosthetic company RSL Stepper has also given in-kind contributions to allow the system to be demonstrated on their Bebionic prosthetic hand. Possible venues of commercialization in this regard are also being considered.\n Future work could involve synergizing tongue movement and neural modes of interface to develop a cohesive, robust human/robot interface. A foundation for two distinct modes of operation is envisioned whereby several devices (e.g. a power wheelchair, prosthetic limbs, household appliances, stationary mechanical assist devices, etc.) may all be directed given the possibilities for control input. Electronics,Healthcare","dataset":"gtr"}