{"id":"EFCECF9D-F989-4601-BBEA-13B6112FF981","title":"Scalable Information Fusion: Adaptivity for Complex Environments and Secure Data","abstractText":"Visual analysis by human operators or service personnel is widely acknowledged to benefit from a fused representation, where images or video information from different spectral bands are combined into a single representation. To provide maximum utility fused data, or its constituent components, must be delivered in a timely manner, must facilitate simple and flexible processing and must be robust to loss and network congestion.Non infrastructure-based Mobile Ad-Hoc Networks are emerging as suitable platforms for exchanging and fusing real-time multi-sensor content. Such networks are characterised by the highly dynamic behaviour of the transmission routes and high path outage probabilities. They exemplify the type of complex, heterogeneous end-end transmission environments which will be commonly encountered in future military scenarios. The low-bandwidth, noisy nature of the physical channel in many sensor networks represents the most serious challenge to implementation of the digital battlefield of the future. One of the key challenges in such complex networking environments is the need to reliably transport and fuse real time video. Video is acknowledged to be inherently difficult to transmit and this is compounded by the need to support multiple sources to aid fusion and situational awareness while maintaining data security. We will focus our work on embedded video bitstreams (MPEG-4 (SVC) which offer scalability and enhanced flexibility for adaptation to varying channel types, interference levels, network structures and content types. These mitigate the need for highly inefficient video transrating processes and instead present a more tractable requirement in the form of dynamic bitstream management.A multisource approach to streaming is proposed which will support video fusion in a bandwidth-efficient manner while having the potential to significantly increase the robustness of real-time transmission in complex heterogeneous networks. Source coding and fusion will be based on the concept of scalability using an embedded bitstream. This means that the source need only be encoded once and that the coded representation can be truncated to support multiple diverse terminal types and to provide inherent congestion management without feedback. Such a system must be designed to maintain optimum fusion performance and hence intelligibility in the presence of bitstream truncation. The potential advantages of this scheme include:- A joint framework for scalable fusion and compression supporting both lossless and lossy representations. - Flexibility for optimisation depending on content type and application.- Graceful degradation: the capability of the fused video bitstream to adapt to differing terminal types and dynamic network conditions - Error resilience: the structure of the code stream can aid subsequent error correction systems alleviating catastrophic decoding failures.- Secure delivery: the ability to design encryption schemes which support truncation.- Region-of-Interest coding: supporting definition of ROIs for priority transmission.","grantUrl":"http://gtr.rcuk.ac.uk/projects?ref=EP/H012710/1","grantId":"EP/H012710/1","fundValue":"103458","fundStart":"2010-01-01","fundEnd":"2011-05-31","funder":"EPSRC","impactText":"  The work was performed in collaboration with General Dynamics who performed an assessment of the technology. The constraint on deployment was the use of a non-standardised codec.  ","person":"David  Bull","coPersons":["Alin Marian Achim","Nishan  Canagarajah"],"organisation":"University of Bristol","findingsText":" This project introduced a new framework for joint scalable compression and fusion of video content in the compressed domain. Firstly, it demonstrated that compressed domain fusion is impossible for conventional video codecs due to drift introduced by multiple sets of interacting prediction loops. The proposed framework overcomes this by using a prediction-free compression technique based on a 3D Dual-tree Discrete Wavelet Transform (3D-DDWT) together with iterative noiseshaping, a novel non-expa The findings would be applicable in defence or security applications where communication of multi-sensor data occurs over a network with dynamic bandwidth variations. In such cases it is often necessary to change the video bandwidth and retain security. Conventionally this is done by decrypting and then transcoding and then re-encrypting. Our system enables, for the first time, multi-source video fusion with the capability to create an embedded and quality-scalable bitstream, thus facilitating Aerospace, Defence and Marine,Digital/Communication/Information Technologies (including Software),Healthcare,Security and Diplomacy,Transport","dataset":"gtr"}