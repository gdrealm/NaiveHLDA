{"id":"9AD59D47-BEF7-4E24-BCD8-8C0AA850E96C","title":"General-Purpose Learning Algorithms for Spiking Neural Networks","abstractText":"How does the nervous system work? How does a cognitive system learn? And how is high-level human or animal learning related to changes in the nervous system? This research project will contribute to these research questions in context of research areas Computational Neuroscience, Cognitive Science and Machine Learning. We want to develop general-purpose learning algorithms for spiking neural networks. Such learning algorithms are of great importance for their potential to tie together complementary approaches towards learning on the neuronal and cognitive level and could lead to a major break-through towards a unified understanding of learning and information processing in Computational Neuroscience and Cognitive Science. The aim of Computational Neuroscience is to understand the detailed computational properties of nervous systems and build artificial neural network models that are biologically plausible, i.e. that model the function of a real neural network (in the brain) as closely as possible with an artificial neural network (on the computer). In contrast, Cognitive Science looks at (human, animal or even artificial agent) cognitive behaviour from a more global point of view and tries to draw conclusions about the underlying mechanisms of information processing in the brain. Models for such processing are often inspired by neural models, but not necessarily biologically realistic, and it is an open problem how properties of cognitive systems are grounded in properties of neural systems. On the cognitive (and also the technical) level, learning is often target-driven: a system needs to achieve a certain task, and gets feedback about how well it is doing. Based on this feedback, its behaviour is changed. Such learning also often involves inferring a priori arbitrary relations in the data given to the system. On the neural level, there are the neurons and their connections (synapses), and neuroscience has observed a number of ways in which these change when a system learns. It is however unknown how feedback on performance on the global level is broken down into localised changes to neurons and synapses on the neural level in a functional way and how known mechanisms of adaptability on this neural level conspire so that on the high-level goal-oriented learning emerges.More specifically, we want to develop learning algorithms for artificial networks of spiking neurons that make use of known neural processes of adaptability in a way such that the networks are able to learn tasks in a goal-oriented, target-driven way. Furthermore algorithms shall allow for networks to develop internal representations of a task which can be analysed and conclusions drawn from about human or animal information processing in a similar cognitive task. The project will deliver a series of learning algorithms for artificial networks of spiking neurons that are general-purpose (that is, not tied to a specific task but able to learn arbitrary input-output relationships), supervised and biologically plausible. No such algorithms exist so far. The research will have significance for the following:1. It grounds higher-level learning in low-level neural adaptability.2. The project can trigger experiments into a novel combination of learning mechanisms in the nervous system.3. It can bring forward the interpretation of the neural code through analysis of internal network dynamics in response to a learnt task.4. Models of neural systems are of interest as learning devices in their own right with a range of applications in artificial intelligence. New learning algorithms for artificial neural networks can bring forward the quest for intelligent computers.5. The research can contribute to understanding the (mal)functioning of the nervous system better, and it could consequently have a long-term impact on the medical sciences for curing neuronal disorders.","grantUrl":"http://gtr.rcuk.ac.uk/projects?ref=EP/I014934/1","grantId":"EP/I014934/1","fundValue":"100751","fundStart":"2011-03-07","fundEnd":"2012-04-06","funder":"EPSRC","impactText":"  The research funded in this grant was fundamental research exploring the functioning of artificial spiking neural networks as models of the brain. Hence impact was expected and has been released mainly within the academic community -- example through follow up funding from the the Human Brain Project. Other Cultural","person":"Andr√©  Gruning","coPersons":[],"organisation":"University of Surrey","findingsText":" Our aim was to contribute to bridging the gap between cognitive modelling and neuroscience through provision of supervised learning algorithms for artificial networks of spiking neurons that make use of neural processes of adaptability such that they are able to learn tasks in a goal-oriented, target-driven way. Such learning algorithms can tie together approaches towards learning on the neuronal and cognitive level. The project focused on two complementary routes:\n1. making use of reservoir computing and STDP.\n2. making use of gradient-descent and error back-propagation techniques.\n\n1. The Reservoir Computing Route\n\nWe explored how to shape the dynamics of a reservoir of spiking neurons using different versions of STDP and then optimise the dynamics for learning relations between spike train inputs and desired firing patterns of output neurons. \n\nChrol et al (2012) concentrate on exploring how STDP can be utilised to create rich dynamics in a reservoir maximising the number of polychronous groups which serve as dynamic patterns on which readout neurons could latch. \n\nYusoff et al (2012) demonstrates that in a supervised learning scheme based on rewarded-modulated STDP a reservoir can detect sequences of input patterns and fire subgroups of neurons in response to different inputs. Outputs are successfully trained on a temporal exclusive-or problem which is an important nonlinearly separable benchmark problem.\n\nNotely and Gruning (2012) presents a milestone for supervised learning algorithms for spiking neural networks. We combine concepts from reservoir computing, synaptic plasticity and gradient descent learning and explore how different synaptic plasticity rules shape the dynamics of the reservoir and facilitate supervised learning with the ReSuMe rule in a spatio-temporal spike pattern transformation task. The major result is that a reservoir with a spatial structure and a tri-phasic STDP rule supports learning best. A journal article with full results is currently in preparation.\n\n2. The Error Back-Propagation Route\n\nIn Gruning and Sporea (2011) we demonstrate for the first time that spiking networks can learn logical operations in a supervised way if data are encoded as fully spatio-temporal spike patterns, using a novel combination of gradient-descent algorithms with synaptic scaling. Its main result for spiking neurons mirrors the classical result for rate neuron perceptrons that the exclusive-or operation cannot be learnt without a hidden layer. \n\nSporea and Gruning (2012a,b) we develop an extension of the ReSuMe learning algorithm to spiking networks of multiple layers. It overcomes limitations of existing learning algorithms as it can be applied to neurons firing multiple spikes. The algorithm is applied successfully to the XOR problem, the Iris data set and more complex (noisy) pattern transformation problems. This is another milestone towards a general-purpose supervised learning algorithm for spiking neural networks. 1. From the results on how neuronal mechanism can be utilised for higher-level learning, a direct application could be in so-called direct brain interfaces that learn how to interpret the neural code to actuate any prosthetics or even games and vice-verse to produce artificial neural signals hat can be understood by the nervous system. A direct brain interface relies on the direct communication of electronic hardware and biological wetware. Hence learning at the interface between the two has to take natural neuronal learning seriously and tune the electronics accordingly. \n\n2. An enhanced learning algorithm for artificial neural networks can be used to improve artificial intelligence. Current artificial intelligence has made impressive advances but is generally far from human or animal performance levels for many real-life problems. Results of the conducted research contribute to understanding neural and cognitive processing better, and hence can make it easier to reproduce similar processing in computer systems with a more human-like intelligence. Digital/Communication/Information Technologies (including Software),Electronics,Other","dataset":"gtr"}