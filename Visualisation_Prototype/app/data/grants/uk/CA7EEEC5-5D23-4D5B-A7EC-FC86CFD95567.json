{"id":"CA7EEEC5-5D23-4D5B-A7EC-FC86CFD95567","title":"Bioinspired vision processing for autonomous terrestrial locomotion","abstractText":"Land vehicles have been designed almost exclusively to use wheels whereas terrestrial animals almost exclusively use legs for locomotion. Wheeled systems can be fast and efficient on hard flat ground; leg based systems are more versatile and efficient on natural terrain. As we move towards a future of autonomous systems operating beyond the extent of the road network and on other planets it is likely that development of robust artificial leg-based locomotion will become increasingly important.\nAt present, several limits of technology prevent the emergence of autonomous legged systems with biocomparable performance. Even if a system was to emerge that could walk, run, leap, and turn without falling over, the technology does not exist safely to guide it through complex terrain using vision. Typically research into using vision for autonomous locomotion is undertaken using available vehicle technology - suggesting that the emergence of high-performance, vision-guided legged systems might occur at some time following the emergence of a basic high performance legged vehicle platform. In a novel approach we will expedite the development of a vision control architecture for locomotion over complex terrain by using human subjects as high performance vehicle platforms.\nThe visual scene captured using a head mounted camera will be processed to identify terrain characteristics known to be important for control of locomotion. A map of the terrain synthesised in 3D virtual space and updated in real-time is presented to the human using a virtual reality headset. The overall outcome measure will be the locomotion performance achieved by the humans using the system compared to that with no vision information available and with normal vision.\nThere are many benefits of this approach: it will allow us to investigate how humans modulate gait paramters and limb mechanics to compensate for partial or unreliable inforamtion about the environment. It will provide insight into the integration of feedforward and feedback control of locomotion. It will allow us to determine the locomotion performance that is possible from a given amount and quality of visually derived information given a highly developed locomotor platform and thus to understand how these two components of a high performance locomotor sytem combine to determine overall performance.\nThe basic principles and technologies establilsed during this project will be applicable to any land vehicle whether based on wheels or legs. Additionally, the processing of visual information for locomotion control is a special case of the more generalised task to search the ground for an object or visual feature. The technology developed in this project may be translated to other applications in which visually-guided autonomous function is required.","grantUrl":"http://gtr.rcuk.ac.uk/projects?ref=EP/J012025/1","grantId":"EP/J012025/1","fundValue":"548721","fundStart":"2012-08-24","fundEnd":"2015-08-23","funder":"EPSRC","impactText":"  As of November 2014 our activities have been focussed on research and technical development some of which has generated papers either published or in review. We are aware of several potential applications for our findings and anticipate these will develop in the future.  ","person":"J  Burn","coPersons":["David  Bull","Iain Donald Gilchrist","Walterio Wolfgang Mayol-Cuevas"],"organisation":"University of Bristol","findingsText":" As of November 2014 we have developed a prototype system that presents a human with a synthesised version of the visual environment in realtime using a single head-mounted camera to collect visual information and a VR headset. The video stream from the camera is split into two processing pathways which construct estimates of the shape of the environemnt and the materials from which it is made. We have shown that the performance of vision systems used controlling terrestrial locomotion can be optimised by satisfying certain relationships between basic system paramters such as camera height above ground, resolution, angle of view etc.\nNOTE this is an interim report and the information provided below is in the context of 'work completed to date'. the principle areas of application for our findings will be:\n1) in medical research to develop tools to assist the partialy sighted\n2) by autonomous systems engineers to control vehicles\n3) by vision scientists to research the human vision systems Aerospace, Defence and Marine,Construction,Digital/Communication/Information Technologies (including Software),Healthcare,Culture, Heritage, Museums and Collections,Pharmaceuticals and Medical Biotechnology,Transport","dataset":"gtr"}