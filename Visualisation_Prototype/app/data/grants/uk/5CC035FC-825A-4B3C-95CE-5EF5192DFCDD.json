{"id":"5CC035FC-825A-4B3C-95CE-5EF5192DFCDD","title":"PrivInfer - Programming Languages for Differential Privacy: Conditioning and Inference","abstractText":"An enormous amount of individuals' data is collected every day. These\ndata could potentially be very valuable for scientific and medical\nresearch or for targeting business. Unfortunately, privacy concerns\nrestrict the way this huge amount of information can be used and\nreleased. Several techniques have been proposed with the aim of\nmaking the data anonymous. These techniques however lose their\neffectiveness when attackers can exploit additional knowledge.\n\nDifferential privacy is a promising approach to the privacy-preserving\nrelease of data: it offers a strong guaranteed bound on the increase\nin harm that a user I incurs as a result of participating in a\ndifferentially private data analysis, even under worst-case\nassumptions.\n\nA standard way to ensure differential privacy is by adding some\nstatistical noise to the result of a data analysis. Differentially\nprivate mechanisms have been proposed for a wide range of interesting\nproblems like statistical analysis, combinatorial optimization,\nmachine learning, distributed computations, etc. Moreover, several\nprogramming language verification tools have been proposed with the\ngoal of assisting a programmer in checking whether a given program is\ndifferentially private or not.\n\nThese tools have been proved successful in checking differentially\nprivate programs that uses standard mechanisms. They offer however only a\nlimited support for reasoning about differential privacy when this is\nobtained using non-standard mechanisms. One limitation comes from the\nsimplified probabilistic models that are built-in to those tools. In\nparticular, these simplified models provide no support (or only very\nlimited support) for reasoning about explicit conditional\ndistributions and probabilistic inference. From the verification\npoint of view, dealing with explicit conditional distributions is\ndifficult because it requires finding a manageable representation, in\nthe internal logic of the verification tool, of events and probability\nmeasures. Moreover, it requires a set of primitives to handle them\nefficiently.\n\n\nIn this project we aim at overcoming these limitations by extending\nthe scope of verification tools for differential privacy to support\nexplicit reasoning about conditional distributions and probabilistic\ninference. Support for conditional distributions and probabilistic\ninference is crucial for reasoning about machine learning\nalgorithms. Those are essential tools for achieving efficient and\naccurate data analysis for massive collection of data. So, the goal of\nthe project is to provide a novel programming language technology\nuseful for enhancing privacy-preserving data analysis based on machine learning.","grantUrl":"http://gtr.rcuk.ac.uk/projects?ref=EP/M022358/1","grantId":"EP/M022358/1","fundValue":"91961","fundStart":"2015-08-01","fundEnd":"2015-12-31","funder":"EPSRC","impactText":"","person":"Marco  Gaboardi","coPersons":[],"organisation":"University of Dundee","findingsText":"","dataset":"gtr"}