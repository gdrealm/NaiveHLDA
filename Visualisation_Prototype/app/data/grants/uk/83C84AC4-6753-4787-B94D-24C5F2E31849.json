{"id":"83C84AC4-6753-4787-B94D-24C5F2E31849","title":"LILiR2 - Language Independent Lip Reading","abstractText":"It is known that humans can, and do, lip-read but not much is known about exactly what visual information is needed for effective lip-reading, particularly in non-laboratory environments. This project will collect data for lip-reading and use it to build automatic lip-reading systems: machines that convert videos of lip-motions into text. To be effective such systems must accurately track the head over a variety of poses; extract numbers, or features, that describe the lips and then learn what features correspond to what text. To tackle the problem we will need to use information collected from audio speech. So this project will also investigate how to use the extensive information known about audio speech to recognise visual speech.The project is a collaboration between the University of East Anglia who have previously developed state-of-the-art speech reading systems; the University of Surrey who built accurate and reliable face and lip-trackers and the Home Office Scientific Branch who wish to investigate the feasibility of this approach for crime fighting.","grantUrl":"http://gtr.rcuk.ac.uk/projects?ref=EP/E028047/1","grantId":"EP/E028047/1","fundValue":"391814","fundStart":"2007-05-31","fundEnd":"2010-09-29","funder":"EPSRC","impactText":"  Our findings have formed the basis for continued research funding at the University of East Anglia and the University of Surrey. Our objective is to build a pure lip-reading system that can interface with existing language processing systems. Aerospace, Defence and Marine,Security and Diplomacy Cultural,Societal","person":"Richard  Harvey","coPersons":["Barry-John  Theobald","Stephen  Cox"],"organisation":"University of East Anglia","findingsText":" It is known that humans can, and do, lip-read but not much is known about exactly what visual information is needed for effective lip-reading. This project collected new datasets for lip-reading and used these to build automatic lip-reading systems: machines that convert videos of lip-motions into text. It also compared human performance against automatic performance on the same dataset. \n\n\n\nTo be effective at automatic lip reading, systems must accurately track the head over a variety of poses; extract features, that describe the lips and then learn what features correspond to what text. To this end the project developed a state-of- the-art facial feature tracking system that could track any set of facial features on any person, in any environement in real- time. This tracking system has resulted in high quality international publications and interest from a variety of industrial sectors from government through to the post production industries. The project also developed several feature extraction/representations that could be used in the recognition of words and a general recognition framework for lip- reading. It made significant advances in person dependant recognition with accuracies approaching the level of speech recognition and it made significant new progress in the more challenging problem of person independent recognition i.e recognising people speaking who have never been seen by the system before. \n\n\n\nThe project also developed approaches to language identification which allows the recognition of language to be performed just by the motion of the lips and the recognition of expression and non verbal communication, the subtle facial expressions that humans use intuitively to supplement the information provided by a speaker about subtle aspects of communication such as their interest in a topic of conversation. Our findings have been used as input to at least four subsequent grants on lip-reading. \n\nAfter much subsequent work (not funded by EPSRC) we have moved to more robust tracking, off-axis lip-reading, and some evidence of person-independence (the key problem not tackled in this grant). Aerospace, Defence and Marine,Digital/Communication/Information Technologies (including Software),Healthcare,Government, Democracy and Justice,Retail,Security and Diplomacy","dataset":"gtr"}