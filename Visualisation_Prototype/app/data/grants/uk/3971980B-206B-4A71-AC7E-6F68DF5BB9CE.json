{"id":"3971980B-206B-4A71-AC7E-6F68DF5BB9CE","title":"REINS","abstractText":"The REINS project is to design and investigate haptic communicational interfaces (reins) between a human agent and a mobile robot guide. The reins will facilitate joint navigation and inspection of a space under conditions of low visibility (occurring frequently in fire fighting). The focus is on haptic and tactile human robot cooperation as it has been found that a limited visual field and obscured cameras adds to the distress of humans working under pressure. Humans naturally interact with animals using tactile feedback in scenarios such as working with guide dogs and horse riding; the REINS project aims to extend this practice to human robot interaction. Expertise from a number of different disciplines - design, engineering, robotics, and communication - will be brought to bear on the problem of designing a communicational interface which will be both sufficiently robust for the relevant physical environment and sufficiently flexible to allow for the on-the-spot exercise of human judgement and creativity.Inspired by the use of a harness for a guide dog and also the rein to ride or drive a horse, the REINS project will investigate and experiment with haptic interfaces for human-robot cooperation. The low/no visibility constraint ensures the focus is on the tactile and haptic aspects only. Currently, robots do not sufficiently enhance human confidence. In human-robot cooperation, the human (by nature) will try to 'read' the situation, and anticipate the movements of the robot companion. The robot is provided with an impedance filter and the rein enables the human to feel the robot's movements and behaviour. Experiences with remotely controlling a robot which is not directly visible show that 'operators spent significantly more time gathering information about the state of the robot and the state of the environment than they did navigating the robot'.The REINS project aims to map the communicational landscape in which humans (fire fighters, but also the visually impaired) might be working with robots, with the emphasis on tactile and haptic interaction. We adapt a semi-autonomous mobile robot for navigation in front of a human. The robot provides rich sensory data and is enabled to try the mechanical impedances of the objects it encounters. We also design and implement a soft rein (rope), a wireless rein and a stiff rein (inspired by the lead for guide dog) enabling the human to use the robot to actively probe objects. The project thus creates the means to explore the haptic Human-Robot Interaction landscape. We will work from an integrationist perspective in which the communicator is not a mere user of pre-existing signs but a sign-maker; the signs emerge in the ongoing coordination and integration of activities adapted to the particular circumstances. We review the communicational landscape occurring within a team of (human) fire fighters and in addition review literature on working guide dogs and horse riding. A research question is whether the information should be explicitly encoded as messages or can remain implicit.In the initial phase of the project the robot is adapted and the first prototypes of the reins are implemented; the emphasis in this phase is on providing rich data to the human. The second phase is dedicated to surveying the communicational landscape. The human-robot team will navigate a known environment with low visibility where unknown obstacles may occur. At least two different types of reins are applied: one requires that messages are explicitly coded, while the other propagates the information implicitly. Based on experiences in the first trials the reins might be adapted to improve their usability. Professional fire fighters will be the first group of subjects to try the reins, later on also volunteers experienced with guide dogs may join the experimentations.","grantUrl":"http://gtr.rcuk.ac.uk/projects?ref=EP/I028757/1","grantId":"EP/I028757/1","fundValue":"455967","fundStart":"2011-04-01","fundEnd":"2015-03-01","funder":"EPSRC","impactText":"","person":"Jacques Sybrandus Penders","coPersons":["C  Roast","Alexander Heathcliff  Reed","Peter Eland Jones","Alan Franciszek Holloway"],"organisation":"Sheffield Hallam University","findingsText":" The aim of the project is to understand how a human is guided by another intelligent agent when the vision is impaired. In that scenario, haptics becomes a natural solution. Therefore KCL presents identification of abstracted dynamics of human control policies and human responses in haptic perceptions. \nThe SHU team aims at exploring how a human and a semi-autonomous robot can develop cooperation using a haptic interface in low visibility conditions. \n\n1) Human-human guidance (KCL)\n\nFollower-Model Generation\nFirst, Experiments were conducted to extract the control policies in guiding and following when a blindfolded human is guided by another human by using a hard rein. The key findings from human demonstration experiments are, \n? By modelling how state maps into action by a Nth order state dependent discrete linear controller, it was found that the guider gave more emphasis on 3rd order predictive model and the follower gave more emphasis on 2nd order reactive model. The model order and reactive/predictive nature will give an insight of human behaviour in guiding and following when the vision is impaired.\n? Studying of how the guider may modulate the pulling force in response to the confidence level of the follower in order to study how the above control policy interact with the follower in an arbitrary path tracking task is necessary to understand the follower's trust on the guider. Modelling the voluntary movements of the follower as a damped inertial system. It was found that damping coefficient is most sensitive to explain the confidence level of the follower.\n\nFollower-Model Validation \nSecondly, extracted control policies were implemented on planner 1-DoF robotic arm to test its validity and stability. Moreover the human response patterns and behavioural characteristic were studied when the blindfolded human's most dominant arm was perturbed by the robotic arm to guide them in to a desired angular position. They key findings from human robot demonstration experiments are,\n? Studying the human movements in leftward and rightward directions convey how humans understand the perceiving information to move another point when the robotic arm uses to generate a tug force to perturb the humans' most dominant arm in low visibility conditions. The asymmetry was noticed in human behavioural metrics in leftward/rightward movements. The extracted behavioural metrics present characteristics and limitations of human movements when the haptic perturbation is given from different directions.\n? Haptic perception of external perturbations depends on involuntary muscle recruitment patterns to stabilize the hand.\n\n2) Robot-Human Guidance (SHU)\n\nInspired by how a visually impaired person is collaborating with a guide dog, we distinguish between the navigation task and the task of guiding.\nThe guide dog just guides while the human being makes the navigation decisions (which direction to go to reach the goal). In addition, exploration of the (unknown) environment is an additional task in search and rescue.\n\nThe focus of the REINS project is on locomotion guidance - the (autonomous) robot leading the human along a safe path - and the human using the robot for exploring the close by environment.\n\nThe project has experimented with 1) a wireless rein, 2) a flexible rein (rope) and 3) a stiff rein.\nA basic requirement for guidance as well as exploration is that the human being 'knows' where the robot is. A stiff rein provides that information implicitly. If a flexible rein is tense it also provides a good feeling (implicit information) of direction however when slack occurs there is no feedback. A wireless rein does not provide any implicit information; all feedback (including the position of the robot) has to be made explicit.\n\nThe project continues to investigate using a fixed rein, which not necessarily requires a set of a-priory fixed codes. We aim for so-called non-performative feedback; we are not looking for coded messages, a coded message would be for example: 'two taps mean go right'. Most promising is to use and apply our findings for devices that are being used in normal conditions (visibility), for instance to assist elderly people. Healthcare,Security and Diplomacy","dataset":"gtr"}