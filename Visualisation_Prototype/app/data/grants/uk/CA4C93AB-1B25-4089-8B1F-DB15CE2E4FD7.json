{"id":"CA4C93AB-1B25-4089-8B1F-DB15CE2E4FD7","title":"Ultrax: Real-time tongue tracking for speech therapy using ultrasound","abstractText":"Speech Sound Disorders (SSDs) are the most common communication impairment in childhood, affecting 6.5% of all UK children, or 2 children in every classroom. SSDs make it difficult for people to communicate with peers and integrate with society, yet the efficacy of interventions for most types of SSDs is weak. Speech, Language and Communication Disorders (SLCD) are a key UK government priority at present, with 2011 designated the national year of speech, language and communication . A recent government report (Bercow, 2008) highlighted the need for a programme of research to enhance the evidence base for children and young people with SLCD. Our programme of research aims to fulfill this need by developing technology which will aid the assessment, diagnosis and treatment of SSDs. Currently in Speech and Language Therapy, technological support is sparse. Technologies that do exist have been expensive to run or complicated to operate and hence not adopted in clinical practice. This project will develop technology (Ultrax) to turn ultrasound into a tongue imaging device specifically designed to provide real-time visual feedback of tongue movements. Most interventions for SSDs rely heavily on auditory skills; clients must listen to their own productions and modify them. However, with Ultrax people with SSDs will actually be able to see the movements of their own tongues and use this information to modify their speech. It is already possible to capture tongue movements by placing a standard medical ultrasound probe under the chin. Ultrasound has the potential to provide powerful information about atypical speech and to enable speakers to modify their own incorrect articulations. However, the image is grainy, information (especially about the tongue tip) is often lost and the image is difficult to interpret. We will improve this image by exploiting prior knowledge about the range of possible tongue shapes and movements in order to provide valuable constraints in tracking tongue contours in sequences of ultrasound images. We will apply a tongue model to this problem, making use of explicit sequence-based optimization for dynamic tracking and smoothing through time. We will use this technology to enhance the ultrasound images, transforming them into a dynamic, real-time 2D video of the tongue's movements which we hypothesize will be A) more easily understood by children B) extend the range of visible tongue shapes from only vowels and /r/ to include /t/,/k/,/ch/ and other consonants which are often targets for therapy Ultrax will be used to provide bio-feedback therapy for people with SSDs and to provide a means for objectively assessing progress by comparing tongue shapes before and after therapy. We will collect a large database of ultrasound and MRI images of tongue movements from 12 adults (ultrasound and MRI) and 90 primary school children (ultrasound) on which to base the model of tongue contours and to test its performance. At the same time, we will split the 90 children into 3 groups and record each group's response to one of 3 types of ultrasound display: 1. Raw, unenhanced, ultrasound 2. Unenhanced ultrasound with added anatomical context (e.g. the position of the teeth and roof of the mouth) 3. Fully enhanced ultrasound, developed in this project. The ability of the children to imitate tongue shapes and movements will be evaluated to determine whether they find the enhanced images easier to interpret than unenhanced images, leading to an improved ability use ultrasound for bio-visual feedback. We will trial ultrasound therapy with 9 children with SSDs (3 children for each type of display) enabling us to evaluate practical issues arising during therapy and pave the way for a future clinical trial. At the conclusion of our research project we will have developed the basis for a new visual-feedback tool (Ultrax) for Speech and Language Therapists to use in the diagnosis and treatment of SSDs.","grantUrl":"http://gtr.rcuk.ac.uk/projects?ref=EP/I027696/1","grantId":"EP/I027696/1","fundValue":"586154","fundStart":"2011-02-01","fundEnd":"2014-07-31","funder":"EPSRC","impactText":"  Articulate Instruments was the commercial partner in the Ultrax project.\n\nDuring the course of the Ultrax project, Articulate Assistant Advanced software was developed by Articulate Instruments Ltd and used by the team to provide speech therapy, record synchronous audio, ultrasound and lip video data, to label target segments, fit splines to MRI data, fit splines to ultrasound data, export raw ultrasound, splines and audio for further processing in Matlab. These enhancements were provided as free upgrades to this commercially available software which is currently used by more than 40 university departments around the world.\nArticulate Instruments Ltd benefits from access to the valuable database of ultrasound and MRI recordings created and annotated during the project. The company looks forward to continuing the close working relationship formed over the course of the project and eventually, it hopes, incorporating a working image enhancement algorithm, based on the scheme developed as part of the project, into the company's software so that it can be of benefit to clinicians and speech researchers working with ultrasound.\n\nSoftware development by Articulate instruments during the project is now available for all users of the software at no extra charge.\n - Revision 213 (17.06.11). Live overlay of palate traces and marker lines on ultrasound display to enhance visual feedback was found to be helpful in making the ultrasound image easier to interpret by speech therapy clients. This feature is available for other labs around the world now using the software in speech therapy research. Improved interactive spline fitting was developed to allow MRI contours to be traced more easily.\n - Revision 214 (02.07.12). A method of exporting of raw ultrasound data matched with associated spline data and audio files.\n - Revision 215 (30.06.14) Addition of &quot;Fiducial&quot; lines to allow measurement of spline displacement, velocity and acceleration along a user defined axis.\n\n\nThe basic research conducted in Ultrax has been translated to a speech therapy context. The first successful treatment using ultrasound imaging was completed in 2011, with the final case completing in 2014 (at the end of the project). Testimonials from clients are available online, and details of the improvements to the clients' speech have been summarised and presented in a research paper currently under review at Clinical Linguistics and Phonetics.\n\nA PhD project at QMU has successfully extended the therapeutic model developed and tested during ULTRAX into the treatment of two other children (with speech problems associated with cleft palate) and successful treatment was demonstrated in 2013. The use of ultrasound tongue imaging was also tested in an Edinburgh cleft-palate speech therapy clinic (2013). Digital/Communication/Information Technologies (including Software),Healthcare Societal,Economic,Policy & public services","person":"Steve  Renals","coPersons":["James M Scobbie"],"organisation":"University of Edinburgh","findingsText":" The ULTRAX project was concerned with the use of ultrasound imaging for speech therapy. There were several findings.\n\n1. Fast and robust tracking of tongue contours in ultrasound video sequence. By integrating global whole-frame features with local edge-gradient features, we implemented a subject-independent system for tracking tongue contour edges in real-time. This approach used a recursive Bayesian framework, and provided an acceptable level of robustness for online visual display. Offline processing of entire recordings can yield results which are more refined, and will provide additional benefit in use-cases which do not require on-line processing to extract tongue contours from ultrasound sequences.\n\n2. Vocal tract shape modelling. Dimensionality-reduction techniques applied to 2D tracings of phonetically diverse midsagittal articulations from MRI scans were used to give a plausible and flexible model of vocal tract shape, which is straightforward to interpret. Furthemore, we have shown it is possible animate such models within the above mentioned recursive Bayesian estimation framework, using articulatory observation data such as ultrasound tongue contours or articulator points tracked using electromagnetic articulography.\n\n3. Improved tongue movement model. Because visibility of the front of the tongue is limited in ultrasound recordings, we have found animating a generic tongue model (derived from 12 adult subject MRI scans) using ultrasound tongue contour observations alone results in apparently over-constrained tongue tip movement. Therefore, moving beyond the originally envisaged scope of the project, we have explored incorporating additional features that can be extracted from ultrasound data which indicate tongue movements in the absence of a clear contour for the tongue tip. Pilot experiments indicate we can indeed achieve greatly increased movement in the animated tongue tip using this approach.\n\n4. Experimental investigation of real-time imaging of tongue movement. We have used real-time ultrasound images of the tongue's midline surface as it moves during speech as real-time feedback for a talker, who was being taught to pronounce new unfamiliar speech sounds. These experiments were motivated by the ultimate aim of using such feedback as an element of clinical intervention to help children with persistent speech disorders. The results showed, as expected, that some sounds are harder than others to acquire successfully, and that human-mediated intervention by the therapist was far more likely to be successful than a mere exposure to acoustic speech sounds or un-mediated ultrasound images. We also were able to clarify different models of intervention which we can use to explore the use of ultrasound in the future in clinical cases. \n\n5. Ultrasound corpus. The 60 children involved in the experiments above were recorded using simultaneous ultrasound of the tongue, audio, and video of the lips producing a wide range of speech sounds and real words, creating a unique new corpus of typical speech in children. These were used in research and also to provide models for the speech therapy component.\n\n6. Ultrasound/MRI corpus. We also collected a matched corpus with ultrasound (and acoustic) data on the one hand and MRI (and acoustic) data on the other, from 12 phonetically trained adult speakers. The vocal tract surfaces for each speaker have been traced for a wide range of speech sounds in each corpus, and this unique resource was used to help estimate the tongue model. We also showed the effects on the location of the tongue during speech production when speakers lie on their backs (in order to emulate the necessary conditions for recording speech in an MRI machine). \n\n7. Speech therapy. Finally, nine courses of speech therapy were undertaken with 8 individual children. These children were all successfully treated and their speech improved. Particularly useful enhancements of the raw image resulting from our project and used during the therapy included the placement on the screen of a model of the speaker's hard palate and the use of the child's own stored productions. A number of developments to the analysis software were made, to assist automated tracking of the tongue surface, for example. 1. Experimental data. The core experimental data collection was accompanied by other speech corpora as noted above, and it is these which are expected to be particularly useful for other researchers. For example, the clinical corpus has already been used by other researchers at QMU, and a Carnegie Trust research project &quot;Seeing Speech&quot; has used some of our MRI corpus recordings as part of online teaching tool for phonetics - as of October 2014, speech sounds on the site had been viewed 127,250 times. \n\n2. Modelling and Algorithms. The recursive Bayesian estimation approach developed for this project has potential for application and impact in several other speech and visual processing problems which can be cast in similar terms of integrating local and global features. Drawing from the findings of this project, Richmond is adapting the approach to the requirements of speech synthesis and pitch tracking.\n\n3. Ultrasound imaging. In addition to speech therapy, ultrasound is used to image tongue movements in such diverse research fields as: linguistic/phonetic investigation (e.g. related to speech production); psychology experimentation (e.g. speech-related cognition, or response timing); and speech technology (e.g. silent-speech interfaces, automatic speech recognition, and acoustic-articulatory inversion mapping). The methods available to track tongue contours in ultrasound video prior to this project were only semi-automatic and slower than realtime, so requiring laborious and costly human effort. Therefore, the development of a fast and robust method to track tongue contour in ultrasound video brings direct and lasting impact to all these and other research areas. Communities and Social Services/Policy,Creative Economy,Digital/Communication/Information Technologies (including Software),Healthcare","dataset":"gtr"}