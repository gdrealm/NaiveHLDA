{"id":"E02EB11E-584F-446F-9E57-5B3DCB592BFC","title":"Adaptive cognition for automated sports video annotation (ACASVA)","abstractText":"The development of a machine that can autonomously understand and interpret patterns of real-world events remains a challenging goal in AI. Humans are able to achieve this by developing sophisticated internal representational structures for object and events and the grammars that connect them. ACASVA aims to investigate the interaction between visual and linguistic grammars in learning by developing grammars in a scenario where the number of different events is constrained, by a set of rules, to be small: a sport. We will analyse video footage of a game (e.g. tennis) and use computer vision techniques to progressively understand it as a sequence of (possibly overlapping) events, and build a grammar of events. We will do a similar audio/linguistic analysis on the commentary on the game. Both of these grammars will be used to build a representational structure for understanding the game. Visual representations are additionally constrained by the inference of game rules so that object-classification mechanisms are preferentially tuned to game-relevant entities like 'player' rather than game-irrelevant entities like 'crowd-member'. We will also investigate how the two modes, sight and sound, can influence each other in the learning process; interpretation of the video is affected by the linguistic grammar and vice versa. Furthermore, this coupling of modes will lead to improved recognition of both audio and video events when the grammars from the video modes are used to influence the audio recognition, and vice versa. The psychological component of the ACASVA correspondingly attempts to learn how these capabilities are developed in humans; how visual grammars are organized and employed in the learning problem, how these grammars are modified by prior linguistic knowledge of the domain, how visual grammars map onto linguistic grammars, and how game rule-inferences influence lower-level visual learning (determined via gaze-behaviour). These results will feedback into the machine-learning problem and vice versa, as well as providing a performance benchmark for the system.Potential beneficiaries of ACASVA (in addition to the knowledge beneficiaries within the fields of science and engineering) include the broadcasting and on-line video search industries.","grantUrl":"http://gtr.rcuk.ac.uk/projects?ref=EP/F069626/1","grantId":"EP/F069626/1","fundValue":"356802","fundStart":"2009-03-16","fundEnd":"2013-03-15","funder":"EPSRC","impactText":"","person":"Stephen  Cox","coPersons":[],"organisation":"University of East Anglia","findingsText":" The ACASVA project is concerned with teaching a computer how to &quot;understand&quot; events by teaching it how to &quot;see&quot; and &quot;hear&quot; a video and its associated soundtrack. Because we need to start with quite simple events that also have a simple &quot;syntax&quot; (ordering), we have been concentrating on videos of tennis games which have a clear set of rules relating to how events develop. We are particularly interested in how knowledge of the rules affects how the events are &quot;seen&quot; and &quot;heard&quot;; for instance, sho  Digital/Communication/Information Technologies (including Software)","dataset":"gtr"}