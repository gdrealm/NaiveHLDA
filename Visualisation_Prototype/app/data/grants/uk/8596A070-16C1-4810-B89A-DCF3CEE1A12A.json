{"id":"8596A070-16C1-4810-B89A-DCF3CEE1A12A","title":"GLANCE: GLAnceable Nuances for Contextual Events","abstractText":"This project will develop and validate exciting novel ways in which people can interact with the world via cognitive wearables -intelligent on-body computing systems that aim to understand the user, the context, and importantly, are prompt-less and useful. Specifically, we will focus on the automatic production and display of what we call glanceable guidance. Eschewing traditional and intricate 3D Augmented Reality approaches that have been difficult to show significant usefulness, glanceable guidance aims to synthesize the nuances of complex tasks in short snippets that are ideal for wearable computing systems and that interfere less with the user and that are easier to learn and use.\n\nThere are two key research challenges, the first is to be able to mine information from long, raw and unscripted wearable video taken from real user-object interactions in order to generate the glanceable supports. Another key challenge is how to automatically detect user's moments of uncertainty during which support should be provided without the user's explicit prompt.\n\nThe project aims to address the following fundamental problems:\n1. Improve the detection of user's attention by robustly determining periods of time that correspond to task-relevant object interactions from a continuous stream of wearable visual and inertial sensors.\n2. Provide assistance only when it is needed by building models of the user, context and task from autonomously identified micro-interactions by multiple users, focusing on models that can facilitate guidance.\n3. Identify and predict action uncertainty from wearable sensing in particular gaze patterns and head motions.\n4. Detect and weigh user expertise for the identification of task nuances towards the optimal creation of real-time tailored guidance.\n5. Design and deliver glanceable guidance that acts in a seamless and prompt-less manner during task performance with minimal interruptions, based on autonomously built models.\n\nGLANCE is underpinned by a rich program of experimental work and rigorous validation across a variety of interaction tasks and user groups. Populations to be tested include skilled and general population and for tasks that include: assembly, using novel equipment (e.g. an unknown coffee maker), and repair tasks (e.g. replacing a bicycle gear cable). It also tightly incorporates the development of working demonstrations.\nAnd in collaboration with our partners the project will explore high-value impact cases related to health care towards assisted living and in industrial settings focusing on assembly and maintenance tasks. \n\nOur team is a collaboration between Computer Science, to develop a the novel data mining and computer vision algorithms, and Behavioral Science to understand when and how users need support.","grantUrl":"http://gtr.rcuk.ac.uk/projects?ref=EP/N013964/1","grantId":"EP/N013964/1","fundValue":"806994","fundStart":"2016-04-04","fundEnd":"2020-04-03","funder":"EPSRC","impactText":"","person":"Walterio Wolfgang Mayol-Cuevas","coPersons":["Casimir  Ludwig","Iain Donald Gilchrist","Dima  Damen"],"organisation":"University of Bristol","findingsText":"","dataset":"gtr"}