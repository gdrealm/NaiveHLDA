{"id":"C0A363B8-4B54-4A92-AAB4-1974BD58C1B1","title":"Unifying audio signal processing and machine learning: a fundamental framework for machine hearing","abstractText":"Modern technology is leading to a flood of audio data. For example, over seventy two hours of unstructured and unlabelled sound-tracks are uploaded to internet sites every minute. Automatic systems are urgently needed for recognising audio content so that these sound-tracks can be tagged for categorisation and search. Moreover, an increasing proportion of recordings are made on hand-held devices in challenging environments that contain multiple sound sources and noise. Such uncurated and noisy data necessitate automatic systems for cleaning the audio content and separating sources from mixtures. On a related note, devices for the hearing impaired currently perform poorly in noise. In fact, this is a major reason why six million people in the UK who would benefit from a hearing aid, do not use them (a market worth &pound;18 billion p.a.). Patients fitted with cochlear implants suffer from similar limitations, and as the population ages more people are affected. \n\nIt is clear that audio recognition and enhancement methods are required to stop us drowning in audio-data, for processing in hearing devices, and to\nsupport new technological innovations. Current approaches to these problems use a combination of audio signal processing (which places the audio data into a convenient format and reduces the data-rate) and machine learning (which removes noise, separates sources, or classifies the content). It is widely believed that these two fields must become increasingly integrated in the future. However, this union is currently a troubled one, suffering from four problems. \n\nInefficiency: The methods are too inefficient when we have vast amounts of data (as is the case for audio-tracks on the web) or for real-time applications (such as is necessary in hearing aids)\nImpoverished models: The machine learning modules tend to be statistically limited.\nUnadapted: The signal processing modules are unadapted despite evidence from other fields, like computer vision, which suggests thatautomatic tuning leads to significant performance gains \nDistorted mixtures: The signal processing modules introduce non-linear distortions which are not captured by the machine learning modules.\n\nIn this project we address these four limitations by introducing a new theoretical framework which unifies signal processing and machine learning. The key step is to view the signal processing module as solving an inference problem. Since the machine-learning modules are often framed in this way, the two modules can be integrated into a single coherent approach allowing technologies from the two fields to be completely integrated. In the project we will then use the new approach to develop efficient, rich, adaptive, and distortion free approaches to audio denoising, source separation and recognition. We will evaluate the the noise reduction and source separations algorithms on the hearing impaired, and the audio recognition algorithms on audio-sound track data.\n\nWe believe this new framework will form a foundation of the emerging field of machine hearing. In the future, machine hearing will be deployed in a vast range of applications from music processing tasks to augmented reality systems (in conjunction with technologies from computer vision). We believe that this project will kick start this proliferation.","grantUrl":"http://gtr.rcuk.ac.uk/projects?ref=EP/L000776/1","grantId":"EP/L000776/1","fundValue":"97101","fundStart":"2013-11-20","fundEnd":"2015-11-19","funder":"EPSRC","impactText":"  The grant is ongoing so this is premature, but the following application areas are being pursued:\n\n1. efficient methods for removing noise from audio and recognising content for tagging video sound tracks (in collaboration with Google)\n\n2. improving hearing devices for the hearing impaired (together with Dr. Robert Carlyon, Prof. Brian Moore and Dr. David Baguley) Digital/Communication/Information Technologies (including Software),Healthcare Societal","person":"Richard Eric Turner","coPersons":[],"organisation":"University of Cambridge","findingsText":" This research grant has only been running for 3 months. However, in that short time we have made the following contributions:\n1. we have shown a theoretical connection between probabilistic machine learning approaches and classical signal processing processing methods which bring these fields closer together and makes it simpler and more efficient to combine methods from both. This has led to better methods for removing noise from audio and filling in missing data. This contribution has been accepted for publication in IEEE Transactions in Signal Processing.\n\n2. we have scaled up a fundamental machine learning tool -- called a Gaussian process -- so that it can handle large scale datasets containing millions of datapoints. Previously these methods were limited to handling ten thousand datapoints. This contribution has been accepted for publication in the Neural Information Processing Systems Conference. We are pursuing the following opportunities:\n-- applying the technology to improving audio uploaded to the internet and automatically recognising content (together with Google) \n-- applying methods to improving hearing devices (including cochlear implants and hearing aids) Digital/Communication/Information Technologies (including Software),Healthcare","dataset":"gtr"}