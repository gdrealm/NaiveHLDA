{"id":"A6A18D6F-0C64-4021-ACE4-F9CD2B342554","title":"Video Annotation Using Human Activities and Visual Context","abstractText":"This proposal aims to develop a system for video annotation, i.e. assigning meaningful semantic labels to video units. As opposed to many previous studies where ad-hoc concepts such as 'grasses', 'sky' and 'explosion' are adopted, we will address the problem from a different perspective by combining human activity and visual context. On the one hand, humans are usually the subjects of video semantics, e.g. the doer of an action. Their presence, activities and interactions are often the key factors to video contents. On the other hand, context, the physical or informative environment or situation where human activities are undertaken, can greatly clarify ambiguity and reduce complexity in video content understanding. Based on our previous work on human face processing and semantic video analysis, we will develop new algorithms and methods for appearance based non-rigid object (e.g. face) tracking, incremental and robust person-specific facial model updating, and unsupervised automatic contextual analysis. The system will detect and track human faces and provide probabilistic descriptions of each individual human face as well as their trajectories in a video. It will also formulate person-specific facial appearance models online by incrementally and robustly updating a generic facial model. Meanwhile, the system will perform unsupervised visual context analysis from low-level features on each video segments.","grantUrl":"http://gtr.rcuk.ac.uk/projects?ref=EP/C007654/1","grantId":"EP/C007654/1","fundValue":"126876","fundStart":"2006-06-01","fundEnd":"2009-04-30","funder":"EPSRC","impactText":"","person":"Yongmin  Li","coPersons":[],"organisation":"Brunel University","findingsText":"","dataset":"gtr"}