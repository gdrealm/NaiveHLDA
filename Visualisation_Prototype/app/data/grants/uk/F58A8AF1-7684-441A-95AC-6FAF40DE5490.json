{"id":"F58A8AF1-7684-441A-95AC-6FAF40DE5490","title":"SHARES - System-on-chip Heterogeneous Architecture Recognition Engine for Speech","abstractText":"The availability of viable, robust speech recognition systems has the potential to revolutionalise the way that people interact with mobile technology. This implies moving beyond simple call home type commands, to being able to dictate arbitrary, extensive e-mails to your mobile device and to reliably and efficiently access its increasingly complex features using natural speech. This will unlock the potential of next generation portable technology to the widest range of potential users in many important application scenarios e.g. for emergency services and military environments as well as time-efficient business and consumer usage. The current issue is, however, that the increasing algorithmic complexity needed to meet user expectations for naturalness and robustness far exceeds the processing and power capabilities forecast for current embedded processor technology. New architectures are therefore needed to radically advance the pace of state-of-the-art recognition technology for mobile and embedded devices.Commercial speech recognition engines for mobile applications are typically small footprint versions of desktop solutions, with the recognition functionality for acceptable quality highly constrained to the processing and power budget available on any given embedded platform. Applications are typically constrained to a few commands and name or song lists. In comparison, state-of-the art research systems on natural unconstrained speech run up to 200-times slower than real-time on 2.8 GHz Xeon processors. In addition, algorithmic research to maintain recognition accuracy in acoustically noisy operating environments, considered essential to widespread adoption of recognition technology, points towards even greater complexity. The gap between algorithmic requirements and the processing and power capability of conventional processor platforms is thus growing even further.For large vocabulary continuous speech recognition (LVCSR) engines, decoding the most likely sequence of words is essentially an extremely large scale search problem over all possible word combinations. To cope with the huge size of the potential search space, search networks created dynamically during decoding were, until recently, considered the only viable approach to realise large vocabulary recognition. Static networks were too big for all but more constrained vocabulary tasks. However, in a significant departure from accepted wisdom, full expansion of large vocabulary static search networks prior to decoding has been importantly demonstrated using the Weighted Finite State Transducer (WFST). The WFST structure creates considerable potential for achieving efficient regularised decoding architectures, which we intend to exploit. To our knowledge, we would be the first to specifically exploit the Weighted Finite State Transducer network decoding framework in novel hardware architectures for low power large complexity speech recognition.","grantUrl":"http://gtr.rcuk.ac.uk/projects?ref=EP/D048605/1","grantId":"EP/D048605/1","fundValue":"503468","fundStart":"2006-10-01","fundEnd":"2010-03-31","funder":"EPSRC","impactText":"  The research has been directly employed in a start-up company proposal called MVR (Mobile Voice Recognition). The company won the Northern Ireland Science Park (NISP) ?25k HiTech competition. Digital/Communication/Information Technologies (including Software) Economic","person":"Roger  Woods","coPersons":["Paul Martin McCourt","John Patrick McAllister","Ming  Ji"],"organisation":"Queen's University of Belfast","findingsText":" The result of the project was the development of a software core that is unique in the fact that it provides desktop performances for large vocabulary speech recognition on embedded platforms with limited resource. This has been made possible by adopting a novel approach where most of the computation is done off-line instead of on-the-fly as it is the case in traditional approaches. The topic formed the basis of the Queen's University Impact document which has been formally published and a Impact talk given to the Public entitled &amp;quot;Local Talent, Global Impact&amp;quot; on 29th May 2013. A direct Proof of Concept (PoC132) was funded by InvestNI resulting in a patent application and a detailed commercialisation study. A number of follow-on contacts have been made with a number of companies. The project won the 2011 HiTech award at the Northern Ireland Science Park (NISP) ?25k awards for the most promising ideas. Digital/Communication/Information Technologies (including Software)","dataset":"gtr"}