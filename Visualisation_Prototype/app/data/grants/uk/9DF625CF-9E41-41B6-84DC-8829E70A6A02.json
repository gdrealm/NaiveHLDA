{"id":"9DF625CF-9E41-41B6-84DC-8829E70A6A02","title":"Optimal Newton-Type Algorithms for Large-Scale Nonlinear Optimization","abstractText":"The research of the numerical optimization community, and hence our work as well, develops tools, underlying theory and techniques, for solving large-scale optimizationproblems as may occur in engineering and science. Real-life applications that can benefit from our work abound. Manufacturers seek maximum efficiency in the design of theirproduction processes. Investors aim at creating portfolios that avoid high risk while yielding a good return. Traffic planners need to decide on the level and ways of routing traffic to minimize congestion. Finding the `best' solution for such processes commonly involves constructing a mathematical model to describe such problems. The resulting models are usually complex and large scale, depending on a large number of parameters. Models with millions of variables and restrictions are not uncommon. It is therefore imperative to implement the model on a computer and to use computer algorithms for solving it. The methods and codes aim to solve the given problem efficiently, and robustly, thus allowing the software to be employed in a variety of contexts and to be run on a diverse range of computers. Since computers cannot solve mathematical problems exactly, only approximately, one of our priorities is to ensure that the solution obtained by applying our algorithms to the models is highly accurate, close to the `true' solution of the problem.Providing theoretical guarantees that the algorithm will successfully solve the user's problem is also crucial. A useful such `safety net' is for example, the provision of global convergence of the algorithm, namely, showing the algorithm will converge to a problem solution irrespectively of the initial guess used to start the algorithm. Establishing thespeed at which the method approaches the solution is important not only to the practitioner, who is keen to have a fast algorithm, but also computationally, since for example, a fast method better prevents the accumulation of numerical errors by the simple fact of taking less time to solve the problem. The direct relation between rate of convergence and time spent solving the problem implies the latter investigation also gives us an indication how long the algorithm will take to solve the problem; we often refer to the latter as an efficiency or complexity question. We have been involved in the development of a new class of methods, Adaptive Regularization with Cubics (ARC), and related software for nonlinear optimization problems, for which efficiency estimates can be given that are better than for other standard methods, such as the classical Newton and steepest-descent methods. This is a first in the context of the problems that interest us, namely functions that `wiggle' a lot, or in standard terms, that may have many local solutions, not just one global optimum. It turns out that not only are these methods theoretically interesting, but also numerically: we implemented our method on the computer and found that it does better than existing methods for this class of problems when tested on some standard test problems. We are excited by these developments and now plan to do a `proper' computer implementation and extensive testing to see how well the ARC method does. Also, we want to see if the ARC efficiency estimate is the best possible; then, no other method could ever perform better than ARC on nonlinear problems. We are also interested in extending ARC so that it can be guaranteed to compute not just a local solution, but the global one, which is a much more challenging task. Finally, we care about the situation when the problem unknowns are restricted to belong to certain possibly-complicated sets; we also intend to develop ARC to take these restrictions into account. If our work is successful, the ARC software will become part of an optimization library, GALAHAD, that is freely available to the research community, and that has been used to solve many applications.","grantUrl":"http://gtr.rcuk.ac.uk/projects?ref=EP/I028854/1","grantId":"EP/I028854/1","fundValue":"103266","fundStart":"2011-09-01","fundEnd":"2013-09-30","funder":"EPSRC","impactText":"","person":"Coralia  Cartis","coPersons":[],"organisation":"University of Edinburgh","findingsText":" 1) We considered smooth nonconvex unconstrained optimization problems and have found that Adaptive Regularization with Cubics (ARC) methods are optimal from a worst-case evaluation complexity viewpoint amongst a large class of second-order methods, thus being more efficient than Newton's method and other state-of-the-art methods such as trust-region and line-search, for generating an approximate local solution of the problem.\n\n\n\n2) We have analysed the worst-case complexity of constrained optimization problems and were able to devise both first- and second-order methods for this class that have the same worst-case evaluation complexity as in the unconstrained case, a surprising find given that constrained problems are generally more challenging to solve than unconstrained ones. The second-order methods we developed in this context extend ARC algorithms to the constrained case.\n\n\n\n3) Using key ideas from ARC methods, we have developed a novel parallel branch-and-bound algorithm for global optimization problems (where it is crucial to find the global minimum/maximum not just a local solution), called oBB (Overlapping Branch-and-Bound). It employs novel bounding and branching mechanisms, both inspired by the cubic model and its properties and the availability of Lipschitz derivative information. \n\nWe have tested oBB on state-of-the-art test problems with promising results. We have written appropriate documentation and made the resulting code freely available on a dedicated website https://pypi.python.org/pypi/oBB Our software oBB can be downloaded for free from a dedicated website and employed by industry users in need of solving global optimization problems. These problems abound in computational science and engineering, and the demand for efficient software is significant. To make our code more accessible and visible, \n\nwe have had it reviewed for and successfully accepted to become part of COIN-OR,\n\none of the most prominent software repositories in optimization and operations-research that is also open-source. The COIN-OR dedicated page to oBB is\n\n\n\nhttps://projects.coin-or.org/oBB/ Our work on the worst-case complexity of algorithms gives a novel understanding of optimization methods and their efficiency, of interest to both researchers and users of optimization algorithms.\n\n\n\nOur global optimization algorithm oBB can be downloaded freely and used by anyone with a need to solve a small to medium size global optimization problem, in serial or in parallel. This includes both academia and industry users of optimization. As nonlinear optimization is an essential component of computational science, engineering and operations research, the potential use of our code is ubiquitous. To make our code more accessible and visible, we have recently submitted it to be reviewed for and if successful, posted on one of the most prominent software repositories in optimization and operations-research, COIN-OR. Chemicals,Construction,Creative Economy,Digital/Communication/Information Technologies (including Software),Electronics,Energy,Environment,Pharmaceuticals and Medical Biotechnology","dataset":"gtr"}