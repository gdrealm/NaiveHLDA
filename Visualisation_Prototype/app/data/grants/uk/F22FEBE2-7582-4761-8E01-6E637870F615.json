{"id":"F22FEBE2-7582-4761-8E01-6E637870F615","title":"Natural Speech Technology","abstractText":"Humans are highly adaptable, and speech is our natural medium for informal communication. When communicating, we continuously adjust to other people, to the situation, and to the environment, using previously acquired knowledge to make this adaptation seem almost instantaneous. Humans generalise, enabling efficient communication in unfamiliar situations and rapid adaptation to new speakers or listeners. Current speech technology works well for certain controlled tasks and domains, but is far from natural, a consequence of its limited ability to acquire knowledge about people or situations, to adapt, and to generalise. This accounts for the uneasy public reaction to speech-driven systems. For example, text-to-speech synthesis can be as intelligible as human speech, but lacks expression and is not perceived as natural. Similarly, the accuracy of speech recognition systems can collapse if the acoustic environment or task domain changes, conditions which a human listener would handle easily. Research approaches to these problems have hitherto been piecemeal and as a result progress has been patchy. In contrast NST will focus on the integrated theoretical development of new joint models for speech recognition and synthesis. These models will allow us to incorporate knowledge about the speakers, the environment, the communication context and awareness of the task, and will learn and adapt from real world data in an online, unsupervised manner. This theoretical unification is already underway within the NST labs and, combined with our record of turning theory into practical state-of-the-art applications, will enable us to bring a naturalness to speech technology that is not currently attainable.The NST programme will yield technology which (1) approaches human adaptability to new communication situations, (2) is capable of personalised communication, and (3) takes account of speaker intention and expressiveness in speech recognition and synthesis. This is an ambitious vision. Its success will be measured in terms of how the theoretical development reshapes the field over the next decade, the takeup of the software systems that we shall develop, and through the impact of our exemplar interactive applications.We shall establish a strong User Group to maximise the impact of the project, with a members concerned with clinical applications, as well as more general speech technology. Members of the User Group include Toshiba, EADS Innovation Works, Cisco, Barnsley Hospital NHS Foundation Trust, and the Euan MacDonald Centre for MND Research. An important interaction with the User Group will be validating our systems on their data and tasks, discussed at an annual user workshop.","grantUrl":"http://gtr.rcuk.ac.uk/projects?ref=EP/I031022/1","grantId":"EP/I031022/1","fundValue":"6236100","fundStart":"2011-05-01","fundEnd":"2016-07-31","funder":"EPSRC","impactText":"  Our research in broadcast speech recognition is currently being trialled with user partners BBC and Red Bee Media. We are currently discussing deployment with both organisations. We have been awarded an EPSRC Innovation Acceleration Award to integrate our speech recognition technology within the Red Bee Media subtitling pipeline,\n\nOur research in multiparty conversational speech recognition (e.g. meeting transcription) has had commercial and community impact. The research has lead to the formation of Quorate Technology, a company founded in 2012 which aims to improve the way meetings and conversations can be captured, accessed and reviewed. In addition our technology has been taken up by EADS Innovation Works UK.\n\nIn conjunction with MNDA Scotland, we have developed a &quot;voice banking&quot; service containing recordings of several hundred speakers from across Scotland. The main aim of this is to enable accent-specific average voices to be constructed which can then be better adapted to the target speaker, but it also means that donors will have an 'insurance policy' should they ever require a personalised synthetic voice. Voices banked include the First Minister of Scotland, and many MSPs.\n\nOur research in personalised speech synthesis and voice reconstruction has resulted in a collaboration with the Euan MacDonald Centre for MND Research at Edinburgh. The Euan MacDonald Centre was established in Edinburgh, in 2007, by the generosity of MND patient Euan MacDonald and his father, Donald. Initially we carried out a pilot study with Euan MacDonald for whom just three minutes of (disordered) speech was available. We were able to reconstruct a personalised synthetic voice, which is installed on his eye-tracking based AAC device and is in daily use. Euan MacDonald campaigns on behalf of the disabled [B] writing that &quot;I feel that a person's voice is one of the most personal things that they possess and the Voicebank project is another project that I feel passionately about.'' [C].\n\nSince then, in an extended trial, we have successfully provided ten patients with a reconstructed voice that they use via an internet-connected device (e.g. iPad). Current trials involve a prototype user interface in which everything runs locally. This work has had considerable media coverage, for example a special feature in the prime time (9% audience share) Japanese programme &quot;Close-Up Gendai&quot;. The work is being extended into a clinical trial phase supported by an MRC Confidence in Concept award, and funding from the charity MNDA.\n\nThe recently opened Anne Rowling Clinic (founded by donations from J.K. Rowling) will have a recording facility specialized for voice banking purposes, and incorporated into the design as a direct result of our voice banking and reconstruction research.\n\nThe HomeService application aims to take the state-of-the-art speech recognition developed by the NST research team and put it to use in people's homes. For elderly people or people with disabilities who can't or choose not to use conventional means of interacting with technology (such as a keyboard or a computer mouse), speech can be an excellent alternative. This is currently being piloted in a number of people's homes. Aerospace, Defence and Marine,Creative Economy,Digital/Communication/Information Technologies (including Software),Healthcare,Security and Diplomacy Cultural,Societal,Economic,Policy & public services","person":"Steve  Renals","coPersons":["Simon  King","William J Byrne","Phil  Green","Thomas  Hain","Junichi  Yamagishi","Philip Charles Woodland","Mark  Gales"],"organisation":"University of Edinburgh","findingsText":" The aim of the project is to significantly advance the state-of-the-art in speech technology by the recognition and synthesis of natural speech, approaching human levels of flexibility, reliability, and fluency.\n\nWe have made advances in several areas.\n\n1. Learning and adaptation. We have developed new approaches to learning representations for speech and language based on deep neural networks and recurrent neural networks. In contrast to previous approaches, these new approaches require less feature engineering and human design. These approaches have been applied to both speech recognition and speech synthesis. We have also developed new approaches for the adaptation of systems to a new voice, given just a few seconds of speech. We have also developed new factorised modelling approaches which, for example, enable us to separately model the effects of the talker as distinct to the effects of the recording channel.\n\n2. Speech transcription. We have developed several new acoustic modelling techniques: for example, new techniques can model phonetic context in a more efficient way, and a new approach to recognising speech captured using multiple microphones. We have also developed more accurate language models, based on recurrent neural networks, and have introduced a new algorithm to automatically learn a pronunciation lexicon.\n\n3. Speech synthesis. We have introduced new models for synthesising speech based on multiple average voices, and using prior information automatically extracted from talker characteristics. We have developed a new approach ro characterise the perceptual effects of modelling assumptions in speech synthesis through perceptual experiments using stimuli constructed from repeated natural speech. We have developed new techniques for synthesis of conversational speech, for example through automatic pause insertion.\n\n4. Applications. This work has been applied in a number of areas.\n a) transcription of broadcast speech for subtitling, metadata extraction, and archive search. This is in collaboration with user group partners BBC and Red Bee Media\n b) adaptive speech recognition and dialogue management for users with speech disorders, which is currently undergoing trials in users' homes\n c) voice banking and cloning, to create personalised voice output communication aids for people with diseases such as Motor Neurone Disease and Parkinson's Disease. This is also undergoing trials with users. Already our findings are having considerable impact. In particular we have released many of the findings made in the project through open source toolkits (Kaldi, HTK, HTS and Festival) which has resulted in significant take-up. Several of our techniques for speech recognition and speech synthesis are being further developed by other groups. \n\nOur techniques rebelling put to use by several members of the project user group including the BBC and Red Bee Media (broadcast speech transcription); the Euan MacDonald Centre for Motor Neurone Disease Research, and the Motor Neurone Disease Association (voice banking); Quorate Technology (audio search and browsing); Toshiba (speech synthesis); Airbus (speech recognition). Creative Economy,Digital/Communication/Information Technologies (including Software),Education,Healthcare,Culture, Heritage, Museums and Collections","dataset":"gtr"}