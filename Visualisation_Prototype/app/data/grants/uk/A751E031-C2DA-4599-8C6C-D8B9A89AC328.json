{"id":"A751E031-C2DA-4599-8C6C-D8B9A89AC328","title":"Measuring and Enhancing Expressive Musical Performance with Digital Instruments: Pilot Study and Research Workshop","abstractText":"This collaborative project between members of the Centre for Digital Music (C4DM) at Queen Mary, University of London and the multi-institution AHRC Research Centre for Musical Performance as Creative Practice (CMPCP) addresses the following questions:\n\n* How can digital technology help explain what makes a performance expressive?\n* How can an understanding of expressive performance guide the creation of new musical instruments and technologies?\n\nTwo emerging trends, if combined, hold the potential for transformative change in musical performance practice. First, the latest digital musical instruments capture the performer's actions with unprecedented detail, allowing continuous, precise control over every aspect of the resulting sound. Second, the study of musical performance as a creative act has taken a central role in musicology, with performers and scholars producing a vibrant interaction between theory and practice.\n\nMusic and technology have long been linked, but technology alone is not a driver of musical creativity. By extension, more dimensions of control do not make a digital instrument more expressive, and excessively complex interfaces can even become an impediment to expressive performance. The perspectives of performers and performance scholars are required to shape a new generation of digital instruments that are ideally suited to musicians' creative requirements.\n\nOn the other hand, digital technology is indispensable in the measurement and modelling of musical performance. Controlled quantitative studies of performers' actions complement qualitative techniques such as interviews and questionnaires to produce a detailed, balanced picture of performance practice. Digital musical instruments, including traditional acoustic instruments augmented with electronic sensors, provide a valuable data source concerning a performer's physical gestures.\n\nThis project promotes collaboration and knowledge exchange between performance scholars and digital music researchers through two main components:\n\nFirst, a pilot study will be conducted using a sensor-enhanced acoustic piano. The study will focus on the link between expressive intent and physical gesture at the keyboard, and it will serve as a model for future extended cross-disciplinary collaborations. A refereed article will be published on the results, contributing to longstanding debates on the nature of physical keyboard technique (commonly known as &quot;touch&quot;).\n\nSecond, a research networking event will take place as a special paper session of the Computer Music Modeling and Retrieval (CMMR) conference in June 2012. The event will draw academics, postdocs and students from musicological and technological disciplines with the goal of identifying areas of shared interest. A concert performance will be held afterward with submissions invited from composers, performers and musicologists. Performers and scholars will be encouraged to attend both paper session and performance, promoting a wide range of perspectives at each event. Following the event, the project investigators will draft a document outlining potential areas of collaboration emerging from the session.\n\nThe proposed research will be directed by PI Andrew McPherson of C4DM with assistance from a postdoctoral researcher. Co-investigators Elaine Chew of C4DM and Daniel Leech-Wilkinson of King's College London/CMPCP will assist in the design of the pilot study and the organisation of the special session. All members of C4DM and CMPCP will be invited to contribute ideas on how digital technology can model and enhance expressive performance. Long-term impacts emerging from or influenced by this Research Development project include musical instruments that dynamically adapt to the abilities and tastes of the performer, interfaces for non-experts to express themselves musically, new pedagogical techniques and mathematical models of shape, phrasing and gesture in performance.","grantUrl":"http://gtr.rcuk.ac.uk/projects?ref=AH/J013145/1","grantId":"AH/J013145/1","fundValue":"23994","fundStart":"2012-02-14","fundEnd":"2012-09-19","funder":"AHRC","impactText":"  The pilot study of keyboard technique in this project led to new mappings between motion and sound on the TouchKeys keyboard: these mappings let the performer add vibrato and pitch bends to each note in a way that did not interfere with traditional piano technique. This compatibility between existing experience and new techniques made it possible to develop the TouchKeys into a product suitable for a wider performer community.\n\nOver the 2012-2013 period, an improved prototype of the TouchKeys hardware was developed incorporating feedback from user studies (for example: two-dimensional sensing on both black and white keys, and better ergonomics). \n\nIn July 2013, I launched a Kickstarter crowd-funding campaign [1] to produce and distribute TouchKeys instruments to musicians. Both prebuilt instruments and self-install sensor kits were available. The campaign raised over ?46,000, exceeding its goal of ?30,000. Instruments were shipped to musicians in 20 countries. The campaign also generated a large amount of publicity, including 25 media article, 7,000+ Facebook likes and 150,000+ YouTube plays. The publicity led to invited presentations at the IRCAM Forum (Paris, November 2013) and the Innovation in Music conference (York, December 2013).\n\nSince the instruments have shipped, musicians who bought the TouchKeys have begun uploading their own videos. Examples include using it to simulate the string bending found in many rock guitar solos, and splitting the keys into multiple segments by touch location in order to play microtonal Turkish maqam music. \n\nThe software has been released open source (GNU Public Licence), and I am currently exploring a second production run of the TouchKeys hardware, likely to commence in early 2015.\n\n[1] https://www.kickstarter.com/projects/instrumentslab/touchkeys-multi-touch-musical-keyboard Creative Economy,Digital/Communication/Information Technologies (including Software) Cultural,Economic","person":"Andrew Palmer McPherson","coPersons":["Elaine  Chew","Daniel  Leech-Wilkinson"],"organisation":"Queen Mary, University of London","findingsText":" This project built connections among musicians, musicologists, computer scientists and engineers to study expressive musical performance. Instrument design was a particular focus of the project since it draws on each of these areas.\n\nThe project was organised around two activities: an interdisciplinary workshop on expressive performance held at the Computer Music Modeling and Retrieval conference, Queen Mary University of London, in June 2012; and a pilot study of keyboard technique using the TouchKeys multi-touch keyboard technology previously developed by the PI.\n\nThe workshop included 10 speakers from a range disciplines, coming from across Europe and North America. Each presented a short paper followed by a brief group discussion. The workshop was complemented by a three-evening series of concerts featuring new instruments and new performance technologies. These were held at the historic Wilton's Music Hall in London.\n\nThe pilot study of keyboard technique yielded new insights into the physical motion of the pianist's fingers while playing the keyboard: though sound production on the piano itself is percussive in nature (hammers striking strings), the motions needed to play the piano are by nature continuous. Pianists nearly universally agree that the subtle details of physical motion, known as &quot;touch&quot;, are crucial to achieving the right expressive results.\n\nA major finding of the pilot study was how on a new augmented instrument, novel techniques could be integrated alongside familiar ones. Playing the piano is already a complex activity, and it is crucial that any new techniques, such as those made possible by the TouchKeys sensors, do not interfere with traditional playing. We developed new methods of adding vibrato and pitch bends to each note on the keyboard and verified their usability in a study of conservatory pianists. The findings on expressive piano touch hold relevance to piano teachers and students as a way of better understanding the subtleties of motion needed to perform at an expert level. The sensor technology and analytical methods could potentially be deployed in the context of piano lessons, for example through visualisation of how the fingers move on the keys.\n\nInsights into expressive piano touch are also highly relevant to instrument designers, where connecting to a performer's existing experience is important to achieving a good reaction. The more a new instrument can draw on existing technique, the faster the path to expertise on that instrument. Where the new instrument introduces completely new techniques, it is important that they fit with the constraints of traditional performance. Our research yielded insights in this area, and they could be put to use by instrument builders through analysis of our motion data (specifically in the case of the keyboard) or in following similar methodologies (applicable to many instruments).\n\nFinally, the TouchKeys multi-touch keyboard, used for the studies, was itself significantly improved as a result of the project, and it has already had wide-ranging commercial and musical impact. Further details are found in the RCUK Narrative Impact section. Creative Economy,Digital/Communication/Information Technologies (including Software),Culture, Heritage, Museums and Collections","dataset":"gtr"}