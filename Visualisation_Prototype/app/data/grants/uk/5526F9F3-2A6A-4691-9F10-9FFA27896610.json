{"id":"5526F9F3-2A6A-4691-9F10-9FFA27896610","title":"Reconfigurable Autonomy","abstractText":"As computational and engineering applications become more sophisticated, the need for autonomous systems that can act intelligently without direct human intervention increases. Yet the autonomous control at the heart of many such systems is often ad-hoc and opaque. Since the cost of failure in critical systems is high, a more reliable, understandable and consistent approach is needed. Thus, in this project we aim to provide a rational agent architecture that controls autonomous decision-making, is re-usable and generic, and can be configured for many different autonomous platforms. In partnership with the industrial collaborators we aim to show how such &quot;reconfigurable autonomy&quot; can be achieved in relevant applications.","grantUrl":"http://gtr.rcuk.ac.uk/projects?ref=EP/J011916/1","grantId":"EP/J011916/1","fundValue":"405962","fundStart":"2012-06-29","fundEnd":"2016-06-28","funder":"EPSRC","impactText":"","person":"Yang  Gao","coPersons":[],"organisation":"University of Surrey","findingsText":" The research work carried out to date:\n\n1) Used state-of-the art object detection and tracking paradigms that have not received substantial attention in spacecraft navigation applications in the past. More importantly we focused on computer vision techniques that can be as generic as possible (in contrary to engineered models) and can efficiently be integrated into an in-situ reconfigurable architecture. This work specifically focused on saliency based visual modelling techniques that can effectively detect object (e.g., rocks) on a planetary surface. In order to help us develop a sound foundation for visual saliency modelling techniques specifically focused towards autonomous spacecraft navigation (on homogenous planetary surfaces) we started off by carrying out extensive quantitative and qualitative analysis of some of the latest visual saliency models already well known across the computer vision research community. These evaluations were performed on simulated, lab-based and real world recorded datasets that quite closely replicate remote planetary environments. \n\nThis work resulted in the following research findings:\n\n- The use of unsupervised (bottom-up) visual saliency models can indeed prove to be useful in planetary exploration rovers (this may as well be extended to satellite imagery).\n\n- With a remarkable level of distinction among different saliency modelling techniques in terms of their performance measures we are able to differentiate, recognise and understand the crucial sensory excitation parameters that can work better on a homogenous planetary environment.\n\n- This ultimately lead towards a saliency model that is effectively tuned for (the more challenging) homogenous planetary environments\n\n2) Developed a machine vision system for use in nuclear decommissioning process, more importantly for automated classification of nuclear waste material.\n\n- System has real-time processing capabilities\n- Has proven to be robust in visual object detection and recognition The current research work is specifically focused towards more generic machine vision and learning algorithms (being adaptive and robust to novel operating environments) that can potentially lead to a wide variety of applications (but not limited to):\n\n- SAR missions\n\n- International maritime services\n\n- Defence: aerial/ground surveillance\n\n- Medical (such as robotic key-hole surgeries)\n\n- Industrial manufacturing and automation\n\n- Automated nuclear waste disposal and management\n\n- Robotic agricultural plants The current work progress will help us to explore novel dimensions in space robotics in future; for instance autonomous systems that are used for planetary exploration missions can be considered as distributed, cognitive systems (having adaptive, anticipatory and goal directed behaviour) rather than isolated engineered system. Such systems would of course require the use of novel (previously unexplored) unconventional methods in space, such as (but are not limited to): computational intelligence, cognitive vision, cognitive machine learning techniques. Aerospace, Defence and Marine,Digital/Communication/Information Technologies (including Software),Electronics,Energy,Manufacturing, including Industrial Biotechology,Transport","dataset":"gtr"}