{"id":"607CA8D6-4BC1-4470-82A1-F62078EFAB96","title":"Understanding the annotation process: annotation for Big data","abstractText":"Data is being collected and created at the fastest rate in human history; by the far the vast majority of this is in digital format. Allied with this, what was previously &quot;offline&quot; information can now be digitised quickly and cheaply e.g. old manuscripts, maps etc. This vast collection of existing and new information creates new opportunities and also difficulties. For a lot of this information to be useful it must be categorised and annotated in some way, so that sense can be made of the data and also so that the correct data can be accessed more easily. It is possible to complete this categorisation by hand with human annotators, but this effort can be expensive in terms of time, money and resources. This is especially true for large data sets or for data sets that require niche expertise to annotate. With this expense in mind, many have turned to machine learning to annotate data; however machine learning approaches still require human intervention to both create training sets for algorithms and judge the output of algorithms. Thus it is inevitable that human intervention is involved at some stage of the categorisation and annotation process. In this project we aim to gain a better understanding of this annotation process so that we can provide guidelines, approaches and processes for providing the most cost effective and accurate annotations for data sets. \n\nWe propose to work with the three main types of unstructured data faced in big data: text, image, and video. The first challenge is to better understand the process assessors go through when annotating and judging different types of material. This will be carried out using a mixture of qualitative and quantitative techniques, using smaller scale lab-based studies. By better understanding the process by which individuals annotate and classify material, we hope to provide insights which can be used to make the annotation process more efficient, and identify a set of initial factors which affect annotation performance, such as degree of domain expertise and time. Based on this initial work, the aim is to then investigate which of these factors most affect assessment, using large scale crowdsourcing style methods. The final challenge is related to the classification task: how should annotation be approached, to give the best results when used in machine learning? Based on this, the project aims to create a set of guidelines for the creation of annotation and relevance sets.","grantUrl":"http://gtr.rcuk.ac.uk/projects?ref=AH/L010364/1","grantId":"AH/L010364/1","fundValue":"80766","fundStart":"2014-01-01","fundEnd":"2015-06-30","funder":"AHRC","impactText":"  The project is still in the process of generating outcomes. At this point findings have not been used by those outside the project, although discussions with outside research partners are continuing.  ","person":"Robert  Villa","coPersons":["Martin  Halvey"],"organisation":"University of Sheffield","findingsText":" The project is still ongoing, and therefore has not yet produced any findings as projected by the project proposal. The work has so far has, however, thrown up two provisional &quot;findings&quot; which we hope to build on:\n - Via the workshop organised at the ACM SIGIR conference, there was some discussion around augmenting current assessment methods used in Information Retrieval research, in particular recording relevance assessors. Interestingly, the current US based evaluation effort (TREC funded by NIST) are faced with various legal issues with going in this direction, which would suggest that further developments should occur in UK/Europe. \n- Some discussion with project partners, and researchers outside the project, has suggested that relevance judgements made in groups is important (e.g. e-discovery in the legal domain, where groups may judge individual documents for relevance to a court case). While not foreseen as an outcome of the project, work with others outside of the project may move in this direction. A workshop was held at the SIGIR 2014 conference, as outlined in the project proposal's &quot;Pathways to impact&quot; document. It is hoped that outputs from the project and from this workshop, will be utilised by others in the research community. This may include use in evaluation efforts, such as the CLEF (Conference and Labs of the Evaluation Forum). Discussions with CLEF organisers have started on how the project findings can be put to use in future. Government, Democracy and Justice,Culture, Heritage, Museums and Collections","dataset":"gtr"}