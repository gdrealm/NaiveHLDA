{"id":"48623909-8FCB-4A3C-8190-119A308EE8D3","title":"An integrated variant calling pipeline for third-generation sequencing technologies","abstractText":"Rationale The majority of high-throughput sequencing (HTS) applications require two initial processing steps: read mapping, and variant calling. The fast-paced nature of HTS research has led to the development of numerous tools of varying and often ill-understood quality, with usually a narrow application range in terms of sequencing technology and experimental design. This situation will likely not improve with the emergence of 3rd generation platforms (including, for instance, Pacific Biosciences, IonTorrent, Helicos, Oxford Nanopore, Avantome, and Halcyon Molecular). A particular technical challenge for many third-generation HTS platforms is their relatively high indel error rate (inserted or missed bases), compared to 2nd generation (particularly Illumina and SOLiD) technologies, requiring the development of new tools. We have recently developed a read mapper ('Stampy') and integrated SNP and indel caller ('Platypus'), both of which were designed to cope with indel errors and mutations. Recently published and unpublished data shows that our tools are state-of-art for Illumina data. Aims This proposal aims to improve and streamline the initial analysis of HTS data for both existing and future HTS platforms, and to provide relevant tools to the academic community. Building upon our current tool set, it proposes to do this by developing an integrated tool chain, built on solid statistical principles, and applicable to a range of experimental designs. To make the tool chain technology-agnostic, we will develop a generic representation of uncertainty in sequencing reads, implemented as a generic file format for HT sequence reads. Briefly, this representation encodes a sequencer's output as a graphical network (specifically, a transducer), allowing the encoding of the local probability of base mismatches as well as insertions and deletions. This scheme is efficient, and sufficiently rich to represent the error profile of the Illumina, SOLiD and 454 platforms, as well as currently known 3rd generation platforms. Current technologies do not provide rich error models; in particular no current technology annotates reads with per-base indel error rates. To compute these from existing data, as well as to tune factory-provided error models to the particular conditions of a library, lane or flow cell, a recalibration tool will also be developed. Parameterizing the tool chain with the resulting statistical error model allows it to transparently cope with technology improvements, as well as with new technologies provided their error profiles fit within the generic statistical framework. In addition, we will continue to develop the current tool set to cope with a larger range of variants, and widen the spectrum of experimental designs to which it is applicable. Work plan overview We will first show feasibility by aiming at three currently widely used platforms: Illumina and 454 (both available in-house at the WTCHG), and SOLiD (for which we have access to data through the 1000 Genomes project to which both applicants contribute). Following successful development of the tool chain for these platforms, and having established a standard for representing uncertainty in sequence reads, we will adapt these tools for 3rd generation platform. Since the ability to successfully deal with indel errors will be crucial here, we will be helped by our previous experience in developing the read mapper 'Stampy', which shows particularly good sensitivity and specificity for indels.","grantUrl":"http://gtr.rcuk.ac.uk/projects?ref=BB/I02593X/1","grantId":"BB/I02593X/1","fundValue":"626048","fundStart":"2012-04-30","fundEnd":"2016-04-30","funder":"BBSRC","impactText":"  We have developed an in-depth understanding of the error profile of Illumina short read data, and building on that we have developed a variant caller for this data.\n\nThe algorithm has been used widely within the Wellcome Trust Centre for Human Genetics (as standard part of the WTCHG processing pipeline), in in flagship projects such as WGS500 (the success of which led directly to the establishment of Genomics England by the UK government), in consortia (e.g. 1000 Genomes), has generated collaborations (e.g. with Nazneen Rahman, ICR, Sutton), and has contributed to setting up a new company (Genomics Ltd).\n\nThe last collaboration has resulted in a broader, cheaper and faster cancer predisposition test that is now being applied in several NHS centres. \n\nThe company Genomics Ltd is aiming to provide analytical support for the Genomics England 100k Genomes project. Digital/Communication/Information Technologies (including Software),Healthcare,Government, Democracy and Justice,Pharmaceuticals and Medical Biotechnology Societal,Economic,Policy & public services","person":"Gerard Anton Lunter","coPersons":["Gil  McVean"],"organisation":"University of Oxford","findingsText":" We have gained an in-depth understanding of the technical and biological complexities of DNA sequencing data produced by current state-of-art sequencing machines (Illumina HiSeq and MiSeq). Building on this, we have developed analytical software to help analyze this data and allow clinical conclusions to be drawn from it. Our findings will help design future algorithms, and our software will be of direct use to many users of DNA sequencing technology. Agriculture, Food and Drink,Digital/Communication/Information Technologies (including Software),Environment,Manufacturing, including Industrial Biotechology,Pharmaceuticals and Medical Biotechnology","dataset":"gtr"}