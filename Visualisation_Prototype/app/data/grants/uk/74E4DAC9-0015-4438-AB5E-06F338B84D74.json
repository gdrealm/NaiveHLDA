{"id":"74E4DAC9-0015-4438-AB5E-06F338B84D74","title":"Insect-inspired visually guided autonomous route navigation through natural environments","abstractText":"Our overall objective is to develop algorithms for long distance route-based visual navigation through complex natural environments. Despite recent advances in autonomous navigation, especially in map-based simultaneous localisation and mapping (SLAM), the problem of guiding a return to a goal location through unstructured, natural terrain is an open issue and active area of research. Despite their small brains and noisy low resolution sensors, insects navigate through such environments with a level of performance that outstrips state-of-the-art robot algorithms. It is therefore natural to take inspiration from insects. There has been a history of bio-inspired navigation models in robotics but there are known components of insect behaviour yet to be incorporated into engineering solutions. In contrast with most modern robotic methods, to navigate between two locations, insects, use procedural route knowledge and not mental maps. An important feature of route navigation is that the agent does not need to know where it is at every point (in the sense of localizing itself within a cognitive map), but rather what it should do. Insects provide further inspiration for navigation algorithms through their innate behavioural adaptations which simplify navigation through unstructured, cluttered environments.One objective is to develop navigation algorithms which capture the elegance and desirable properties of insect homing strategies - robustness (in the face of natural environmental variation), parsimony (of mechanism and visual encoding), speed of learning (insects must learn from their first excursion) and efficacy (the simple scale over which insects forage). Prior to this we will bring together current insights regarding insect behaviour with novel technologies which allow us to recreate visual input from the perspective of foraging insects. This will lead to new tools for biologists and increase our understanding of insect navigation. In order to achieve these goals our Work Packages will be:WP1 Development of tools for reconstructing large-scale natural environments. We will adapt an existing panoramic camera system to enable reconstruction of the visual input experienced by foraging bees. Similarly, we will adapt new computer vision methods to enable us to build world models of the cluttered habitats of antsWP2 Investigation of optimal visual encodings for navigation. Using the world model developed in WP1, we will investigate the stability and performance of different ways of encoding a visual sceneWP3 Autonomous route navigation algorithms. We will test a recently developed model of route navigation and augment it for robust performance in natural environmentsOur approach in this project is novel and timely. The panoramic camera system has just been developed at Sussex. The methods for building world models have only recently become practical and have not yet been applied in this context. The proposed route navigation methodology is newly developed at Sussex and is based on insights of insect behaviour only recently observed. Increased knowledge of route navigation will be of interest to engineers and biologists. Parsimonious route-following algorithms will be of use in situations where an agent must reliably navigate between two locations, such as a robotic courier or search-and-rescue robot. Our algorithms also have potential broader applications such as improving guidance aids for the visually-impaired. Biologists and the wider academic community will be able to use the tools developed to gain an understanding of the visual input during behavioural experiments leading to a deeper understanding of target systems. There is specific current interest from Rothamsted Agricultural Institute who are interested in how changes in flight patterns affect visual input and navigational efficacy of honeybee foragers from colonies affected by factors like pesticides or at risk of colony collapse disorder.","grantUrl":"http://gtr.rcuk.ac.uk/projects?ref=EP/I031758/1","grantId":"EP/I031758/1","fundValue":"102329","fundStart":"2011-05-01","fundEnd":"2013-04-30","funder":"EPSRC","impactText":"  The grant has had significant academic input with multiple articles, citations and over 10 (and counting)presentations at international conferences.\n\nI have recently (2014) been contacted by researchers in RAL Space and Harper Adams University interested in using the algorithms developed in these grants in Space Exploration and Agricultural robots respectively\n\nBeneficiaries: Biologists and biomimetic engineers\n\nContribution Method: Through publications and presentations\n\nWe have given outreach sessions for schools based on this research and aimed at Widening Participation in particuler. We have done ~15 sessions in total reaching 300 students.\n\nBeneficiaries: schools children\n\nContribution Method: The research was the basis for the demo sessions we have done Digital/Communication/Information Technologies (including Software),Other Societal","person":"Andrew  Philippides","coPersons":[],"organisation":"University of Sussex","findingsText":" We have developed a set of methods for reconstructing the visual input experienced by insects as they forage for food. These methods have been used to gather data sets from the natural habitats of ants during behavioural experiments so we can interpret insect behaviour. In collaboration with Barbara Webb and Michael Mangan (Edinburgh University) and Wolfgang Sturzl (DLR, Munich) we have used these methods to reconstruct the entire visual history of several ants of the whole course of their life outside the nest.\n\n\n\nWe have developed a novel insect-inspired algorithm for autonomous route navigation. This is the first complete model of visual route navigation in ants and is a new approach to modelling visual homing and route navigation. Our algorithm robustly navigates routes through complex environments and shows many characteristics of the behaviour of navigating ants. In particular it can also perform place search with the same mechanism and can explain paradigmatic experiments previously used as evidence for 'snapshot' based models. It has been very well-received by the insect-inspired navigation community. \n\n\n\nWe have analysed a novel innate behaviour: visual scanning in ants. This behaviour prompted our novel navigation model but in turn we have been able to use the model and visual input reconstruction methods to analyse the behavioural consequences of this behaviour in the field.\n\n\n\nWe have undertaken a detailed analysis of the learning flights of bumblebees. We found that the learning flights are composed of nest-centric loops, while return flights are zigzags. WE showed that these different manoeuvres are variants of one another and that the key point of similarity between the two is when the bee both faces and flies towards the nest. This ties in with our familiarity based model of place homing. We also showed how these nest-centric manoeuvres are tied to geocentric information. \n\n\n\nWe have started to analyse learning walks in ants using both the familiarity model and visual input reconstruction methods, through a part-EPSRC funded doctoral student. The main finding is that learning walks should be partly tailored to certain distant features of the world, but that they should also have some general features. The next stage of this work is to tie our simulation results to learning walks in different species of ants in different visual environments that we have recorded. The navigation algorithms are of potential use in autonomous robotic engineering especially in Agriculture and in particular in the navigation of UAVs in GPS-denied environments. \n\n\n\nThere is also potential for the algorithms to be used as navigational aids for visually impaired people. We are in discussions over an EU grant-proposal based around these ideas and also with a company\n\n\n\nOur algorithms will be used for schools outreach and widening participation in particular, as well as public engagement The main outcome of our work is to add to the body of knowledge on visual learning and memory. \n\n\n\n1) It is adding to our knowledge of what is encoded in a visual memory. Our group has a PhD student who will now try and tie this in to recordings from the insect eye to see what visual features are actually encoded\n\n\n\n2) The behaviour is an example of active learning and in particular adds to our knowledge of how innate behaviours can scaffold learning\n\n\n\n3) The scanning behaviour will be used to give us a measure of the ant's uncertainty potentially allowing us to investigate Bayesian combining of navigation cues.\n\n\n\n4) Our image reconstruction methods and navigation algorithms have also been used both by ourselves and other groups to interpret the behaviour of foraging ants thus giving us a window into the algorithms they might use to navigate.\n\n\n\nWe have published multiple papers on this work and are planning to use the methods developed as the basis of grant applications (ERC and HFSP applications were submitted but these schemes are extremely competitive and we were unsuccessful). The work has also prompted new collaborations with Oklahoma University, DLR in Munich and the ANU in Canberra as well as strengthening ties with Edinburgh University.\n\n\n\n \n\nThe other major exploitation route is in engineering and robotics. Our navigation algorithms can be used for autonomous robotic navigation and UAVs in particular. We are testing our algorithm in a variety of natural habitats and contexts and using autonomous robots. For instance:\n\n\n\n(1) we are developing methods by which a flying agent can navigate based on the view of the ground; We are planning grant applications to follow the work up (both EPSRC and EU) in the next 6-12 months.\n\n\n\n(2) we have a prototype of the algorithm that can be run as a mobile phone app which will allow people to recapitulate a path taken by others. We have also had preliminary discussions with a company about exploitation but want to get IP on the guidance algorithm before taking this further.\n\n\n\n(3) Our familiarity based navigation algorithm could also be applied in a more abstract informational context. We are again discussing exploitation with the company in 2.\n\n\n\n\n\nOur algorithms are very simple and attractive to the public and we are using them as the basis of demos for outreach Aerospace, Defence and Marine,Agriculture, Food and Drink,Digital/Communication/Information Technologies (including Software),Transport,Other","dataset":"gtr"}