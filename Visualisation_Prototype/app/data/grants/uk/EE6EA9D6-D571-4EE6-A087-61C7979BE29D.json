{"id":"EE6EA9D6-D571-4EE6-A087-61C7979BE29D","title":"Making sense from the hands and mouth: multimodal integration in spoken and signed languages","abstractText":"<p>Successful communication involves integrating multiple types of information. In sign languages, linguistic information is provided by the hands, but also by the body, face and mouth. In spoken languages, speech sounds are combined with visible mouth movements and gestures. Such information from different sources is tightly synchronised, facilitating comprehension. Most studies of signed and spoken language, however, focus on a single dominant modality (hands or speech), and therefore little is known about how the different types of information are integrated in comprehending language.</p>\n\n<p>This project investigates the integration of mouth and hands in signed and spoken languages, testing how integration affects language comprehension in British Sign Language (BSL) and English, and the extent to which common neural systems subserve integration for signed and spoken languages.This will be achieved by digitally manipulating BSL and English video materials, creating stimuli with mismatches in content or timing of information from the hands and mouth (eg saying &quot;drive&quot; but gesturing &quot;twist&quot;). Behavioural experiments will test the consequences of different types of mismatches on comprehending language; functional MRI studies will investigate which neural systems are involved in integrating information from the hands and mouth in the the two languages. </p>","grantUrl":"http://gtr.rcuk.ac.uk/projects?ref=ES/K001337/1","grantId":"ES/K001337/1","fundValue":"156077","fundStart":"2013-01-01","fundEnd":"2015-10-31","funder":"ESRC","impactText":"","person":"David Patrick Vinson","coPersons":[],"organisation":"University College London","findingsText":"","dataset":"gtr"}