{"id":"1C028C12-B2C5-4149-9D5D-5FD6F0AE848F","title":"OMRAS2: A Distributed Research Environment for Music Informatics and Computational Musicology","abstractText":"Imagine you have just bought a new iPod, you rip loads of your dad's CDs into it (his music's cool) as well as your own, and pretty soon you have 10,000 tracks and the iPod is full. Now there's a problem. You've never listened to your dad's CDs (not that many of them anyway) and you're really not sure what The Human League sounds like, and there's another 500 CDs of his music in there. Where are the good songs? How can you ever build those really cool playlists to impress your friends with your vast musical knowledge?Online Music Recognition and Searching II A Distributed Framework for Music Informatics and Computational Musicology.Imagine you've just been given a gist subscription to a 2 million song online music store. You can choose 10,000 songs to download onto your music player, but there's a problem. You have never heard a vast majority of these songs so you're not sure which are the one's you like. How can you put together those playlists to impress your friends with your vast musical knowledge?The problem is simular for the radio DJ looking for a new playlist to keep their show on the cutting edge, or the professional violinist doing research into different performances of Vivaldi'd Four Seasons to find a new twist for an expectant audience, or the recors producer trying to find a mathimatical formula for number one singles (yes, they really do this).The answer to the above question and other interesting problems concerning large collections of digital music are exactly what the OMRAS2 project will address. When OMRAS2 is completed, you'll be able to get software that helps you build playlists with songs that you'll love even though you never heard them before; and there will be tools to help the violinist and record producer achive their goals too. Using tools from OMRAS2, your ipod will be able to predict the best sounds to use for the best chart topping number one. If you study music at University, you'll probably use OMRAS2 for analysing and comparing music.OMRAS2 aims to help technology researchers build and investigate the software that is needed to construct these super-tools. But that's not all. It will help musci researchers investigate interesting aspect of music, such as what variations of that riff in Purple Haze did Jimi Hendrix play and how did the differ, and how did different pianists interpret Bach's Goldberg Variations. OMRAS2 will also look deeply at how music and information about music (like CD Insert booklets, but more and online) will be enjoyed at home, not just downloading, but also searching, recomending, browsing and so on. And it wont be hard to use:OMRAS2 will use interfaces that look and react like familiar music software like Adobe Audition or RealAudio player.","grantUrl":"http://gtr.rcuk.ac.uk/projects?ref=EP/E017614/1","grantId":"EP/E017614/1","fundValue":"1447590","fundStart":"2007-02-01","fundEnd":"2010-10-31","funder":"EPSRC","impactText":"","person":"Mark  Sandler","coPersons":["Simon  Dixon","Samer  Abdallah","Mark  Plumbley"],"organisation":"Queen Mary, University of London","findingsText":" The project demonstrated:\n\n(1) the utilities of high-level semantic features of musical audio (including musicological terms and free-form labels such as social tags) in multimedia content management,\n\n(2) the use of low-level audio features and probabilistic statistical models to derive high-level semantic descriptors automatically, facilitating navigation in large online audio collections,\n\n(3) the utilities of the Semantic Web, and Semantic Web technologies in online audio content navigation and delivery,\n\n(4) the use of digital signal processing and machine learning for the manipulation of digital audio content on the semantic level, allowing interaction with notes, chords or performance characteristics such as vibrato by re-synthesising audio from parametrised descriptors.\n\n\n\nSeveral computational algorithms were developed for automatic annotation of musical audio, including novel methods for audio transcription, chord recognition, key recognition, tempo and beat detection, structural segmentation and music similarity. \n\n\n\nSeveral Semantic Web ontologies were created for describing and publishing music related metadata on the Semantic Web. The Music Ontology, a core framework connecting the OMRAS2 ontologies became a de-facto standard in music-related data publishing. The are numerous potential uses of OMRAS 2 technologies in a non-academic context. This includes music search engines that are based on audio similarity characterised by acoustic features of sound, search engines and Semantic Web user agents that access audio archives by high level semantic concepts. These include musicological terms such as keys, chords or rhythm, and social-contextual similarity like artist collaborations. These tools can be used in online services, content management platforms, libraries and archives, educational institutions, etc.. Lower level music signal processing tools developed by the project may be utilised in music production and delivery.\n The project developed several easy to use tools allowing the exploitation of project outcomes by academic communities and industry partners. These tools include Sonic Visualiser, and application for examining high or low-level features of sound in the context the audio waveform. Sonic Visualiser is equally useful for researchers working on audio signal processing and machine learning algorithms, as well as musicologists analysing audio collections. The program is supported by a C++/Python Application Programming Interface, that allows distributing content analysis algorithms in a standard plugin format, that can be used by several host applications, including audio editors and batch audio processors. Creative Economy,Digital/Communication/Information Technologies (including Software)","dataset":"gtr"}