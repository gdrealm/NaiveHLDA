{"id":"252AF9B7-A004-4915-B40B-58E3A812EE3E","title":"Multiview Distributed Video Coding For Wireless Multicamera Networks","abstractText":"Continuing advances in hardware technology have led to the emergence of small, low-power devices with limited onboard processing and wireless communication capabilities typically referred to as sensors. A large number of such spatially distributed signal processing devices, each with finite battery lifetime and thus limited computing and communication capabilities, is known as a wireless sensor network. Sensor networks are ideal for situation awareness applications such as environmental monitoring, healthcare monitoring, battlefield and public space surveillance, etc. Video sensor networks are made up of multiple cameras with varying degrees of spatially and temporally overlapping coverage.To fully exploit the potential of such sensor networks, it is essential to develop energy and bandwidth efficient compression algorithms at the individual sensor nodes. Additionally the information loss that is associated with error-prone, wireless environments calls for robust coding methods. All the current video coding standards rely on the hybrid block-based motion compensation/discrete cosine transform architecture which is primarily driven by the one-to-many model of a single complex encoder and multiple light (cheap) decoders, as is the case in the broadcasting scenario. The presence of limited and generally irreplaceable power sources in wireless sensor networks makes the complexity / compression trade off more relevant than pure rate-distortion performance, the latter being normally the goal of these power hungry motion estimation based hybrid codecs. The error prone wireless environment makes robustness a necessary as opposed to a desired characteristic, something that the error propagation prone, temporal prediction based, hybrid codecs cannot easily claim to possess.These new requirements have recently motivated the video coding community to re-visit the information theory principles of Slepian-Wolf and Wyner-Ziv and examine distributed video coding (DVC) systems. Wyner-Ziv coding shifts the complexity - and consequently the power consumption and cost - from the encoder(s) to the decoder(s). DVC systems posses complexity and robustness properties that make then particularly promising for applications with multiple wireless cameras. This project will develop a novel distributed video coding system for multi-camera and multimedia sensor networks that will offer efficient, error resilient and adaptive performance with low complexity / low power encoders under varying content, user and channel conditions. The project will consider all aspects of distributed video coding and will suggest enhanced algorithms with the aim of optimising the performance particularly for the multi-source case. The coding performance and error resilience of the proposed system will be investigated and optimised for wireless networks with the specific application in mind..","grantUrl":"http://gtr.rcuk.ac.uk/projects?ref=EP/E065538/1","grantId":"EP/E065538/1","fundValue":"255729","fundStart":"2007-08-01","fundEnd":"2010-07-31","funder":"EPSRC","impactText":"  Findings from this research have been (and still are) referenced by other researchers Digital/Communication/Information Technologies (including Software) ","person":"Dimitris  Agrafiotis","coPersons":[],"organisation":"University of Bristol","findingsText":" New compression methods for power limited devices/scenarios Researchers can built on the contributions made by this grant to further improve and deploy the investigated compression approach Digital/Communication/Information Technologies (including Software)","dataset":"gtr"}