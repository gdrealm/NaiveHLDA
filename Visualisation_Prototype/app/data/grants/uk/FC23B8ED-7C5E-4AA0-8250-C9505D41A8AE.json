{"id":"FC23B8ED-7C5E-4AA0-8250-C9505D41A8AE","title":"Collaboration with Yaming Yu - entropy inequalities and thinning","abstractText":"Entropy quantifies the way in which, for example, the outcome of tossing a fair coin is harder to predict than with a biased one. It plays a fundamental role in understanding how information is transmitted over noisy communication networks, and how large amounts of information can be stored in as small devices as possible (data compression). Entropy is studied in the field of information theory, and this project aims to resolve two major outstanding conjectures, using techniques from various areas of pure mathematics. These conjectures describe the entropy of the sums of random events. It is a familiar fact that while randomness cannot be predicted, by summing or averaging random events, the unpredictability cancels out. Thus, for example, while a single coin toss is impossible to predict, we can be confident that in 1,000,000 fair coin tosses, there will be between 498,000 and 502,000 heads. This project will consider the behaviour of the entropy in such settings, helping understand such effects.More specifically, many models of information transmission suppose that noise is added to the signal, due to physical processes beyond the control of transmitter or receiver. The ultimate aim is to clean up the received message, that is to remove the noise, in order to receive the full content sent by the transmitter. If we are able to prove the conjectures mentioned above, there would be implications in certain communications networks, including enabling a better quality of live streaming video on the Internet. For example, Stankovic describes a model where such video is broadcast via a wireless link to a number of servers, which compress the information locally. The algorithms which work best are based on the so-called Entropy Power Inequality, which we seek to extend here.","grantUrl":"http://gtr.rcuk.ac.uk/projects?ref=EP/H002200/1","grantId":"EP/H002200/1","fundValue":"7294","fundStart":"2009-06-01","fundEnd":"2011-05-31","funder":"EPSRC","impactText":"","person":"Oliver Thomas Johnson","coPersons":[],"organisation":"University of Bristol","findingsText":" Our work proved fundamental bounds on the entropy of discrete random variables on summation, and gained a new mathematical understanding of the operation of thinning. Such bounds on entropy have applications in Communications, where they can provide fundamental bounds on the performance of communication systems. Such applications are explored within the EPSRC-funded University of Bristol CDT in Communications, within which the grant holder plays a leading part. Digital/Communication/Information Technologies (including Software)","dataset":"gtr"}