{"id":"BB291C36-F31F-4702-8FBC-603EE3ACF228","title":"Corpus-Based Speech Separation","abstractText":"In this project, we will develop new techniques for restoring clear speech from noisy recordings. We will focus on two problems: (1) retrieving speech from background noise, and (2) separating speech sentences spoken by different speakers. For convenience, we reference both problems as speech separation.Over the past decades, there have been many techniques developed for speech separation. While appearing in different forms, most techniques can be viewed as a filter, which aims to pass the frequencies of the targeted speech with minimum distortion, and at the same time block the frequencies of the noise. To build the filter, one thus needs knowledge about the frequency structure of the noise. For certain applications in which the noise remains relatively constant, one may obtain an estimate of the noise structure using the data observed at a time without speech, and then use it to predict the noise structure in the data containing mixed speech and noise. Based on the prediction, a filter can be formed to remove the noise and hence restore the speech. Unfortunately, this strategy does not work if the noise changes fast and thus is unpredictable. Examples of fast-varying noises include crosstalk speech, and the background noises in mobile/Internet communications, which are often complex, highly dynamic, and thus difficult to predict.In this research, we will investigate a new method to speech separation, aiming for the capability of handling unpredictable noise. We will use a pre-recorded speech corpus, consisting of clean speech sentences by various speakers, to help remove the requirement for information about the noise. The new method consists of four major components. First, we compare the noisy sentence, containing mixed speech and noise, with each corpus sentence to find all their matching parts. Second, we combine the longest matching parts from the clean corpus sentences to form a new sentence, as a reconstruction of the target speech. Because of the richer and more distinct contexts, longer speech utterances are less confused by noise, and thus can be recognised with fewer errors than shorter utterances. This explains why we synthesise the target speech using longest recognised speech parts, which minimises the effect of noise on the restoration. The third component of our method is a novel technique to reduce the sensitivity to noise for finding the matching speech parts between the noisy and corpus sentences. The last component uses the speakers characteristics, associated with the individual corpus sentences, to help separate mixed sentences spoken by different speakers. Combining these components, the new method offers the capability to separate speech from noise, and separate mixed speech sentences, without having to predict the noise/crosstalk.","grantUrl":"http://gtr.rcuk.ac.uk/projects?ref=EP/G001960/1","grantId":"EP/G001960/1","fundValue":"312394","fundStart":"2008-10-01","fundEnd":"2011-09-30","funder":"EPSRC","impactText":"  Our research findings from this research have led to impact on a range of different fronts, mainly:\n(i) Collaboration with CSR, a leading $1 billion consumer electronics company, has shaped its R/D research agenda in speech enhancement, has inspired ideas for new product improvements, and has helped establish Belfast as an audio research centre of excellence within the company.\n\n(ii) Collaboration with Vitalograph Ltd, a company delivering healthcare monitoring systems, has led to a robust, acoustic-based monitoring system for medical inhalers, with potential for multi-million pound savings in NHS budgets.\n\n(iii) NTT (Nippon Telegraph &amp;amp;amp; Telephone Corp.) used our method in their speech recognition entry to the International Competition for Machine Listening in Multisource Environments (CHiME 2011), in which they took 1st place. Digital/Communication/Information Technologies (including Software),Healthcare Economic,Policy & public services","person":"Ming  Ji","coPersons":["Danny  Crookes"],"organisation":"Queen's University of Belfast","findingsText":" In the real world, speech rarely occurs in isolation, and is usually accompanied by other acoustic interference. The two most common scenarios are: (1) speech is accompanied by some background noise, e.g., cocktail party noise, background music, street noise or any other environmental noise, and (2) one speaker's voice is masked by other speakers' voices, which happens when two or more people speak simultaneously. Severe interference can make speech unintelligible. Restoring clear speech from no (1) NTT (Nippon Telegraph &amp;amp;amp;amp;amp;amp;amp; Telephone Corp.) used our method in their speech recognition entry to the International Competition for Machine Listening in Multisource Environments (CHiME 2011), in which they took 1st place. NTT has further exploited our method to handle speech de-reverberation (Interspeech'2011).\n\n(2) CSR has rated 5 out of 5 for the significance of the outcomes of the joint KTS project to their organisations future performance. \n\n(3) A patent was filed in 24 August 2012, with the International Application No. PCT/EP2012/066549.\n\n(4) The paper we published in Interspeech 2010, describing our key findings of the research, was selected as the best paper in speech enhancement. Digital/Communication/Information Technologies (including Software),Healthcare","dataset":"gtr"}