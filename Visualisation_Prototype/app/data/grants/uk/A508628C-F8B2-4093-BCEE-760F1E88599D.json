{"id":"A508628C-F8B2-4093-BCEE-760F1E88599D","title":"Information geometry of graphs","abstractText":"Entropy quantifies the way in which, for example, the outcome of tossing a fair coin is harder to predict than with a biased one. It plays a fundamental role in understanding how information is transmitted over noisy communication networks, and how large amounts of information can be stored in as small devices as possible (data compression). More recently, through the emerging field of Information Geometry, it has become clear that entropy can provide understanding of more fundamental questions of statistical inference. Specifically, Information Geometry offers a way to define a `distance' between distributions of random events, giving an unambiguous way to decide how different two models of randomness really are. However, these results are generally only understood in the context of real-valued random events, whereas many random events (those to do with counting, for example) take values in just the set 0,1,2,.... We propose to combine the expertise of the PI in the field of Information Theory with the RA's background in functional analysis, in order to define distance measures in a similar way for these counting processes, and to understand the properties of the resulting measures.","grantUrl":"http://gtr.rcuk.ac.uk/projects?ref=EP/I009450/1","grantId":"EP/I009450/1","fundValue":"179771","fundStart":"2011-09-01","fundEnd":"2013-08-31","funder":"EPSRC","impactText":"","person":"Oliver Thomas Johnson","coPersons":[],"organisation":"University of Bristol","findingsText":"","dataset":"gtr"}