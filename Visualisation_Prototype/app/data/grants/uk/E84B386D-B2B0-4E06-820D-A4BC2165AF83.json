{"id":"E84B386D-B2B0-4E06-820D-A4BC2165AF83","title":"Identifying the signal in the noise: a systems approach for examining invariance in auditory cortex","abstractText":"We are able to recognize and understand speech across many different speakers, voice pitches and listening conditions. However, the acoustic waveform of a sound (e.g. the vowel 'ae') will vary considerably depending on the individual speaker, and the 'ae' may be embedded in a cacophony of other, background sounds in our often noisy environments. Despite this, we have no difficulty recognizing an 'ae' as an 'ae', suggesting that the brain is capable of forming a representation of the vowel sound which is invariant to these 'nuisance' variables. For vowel sounds, the timbre, or vowel identity, is determined by the spectral envelope. Filtering by the mouth, lips and tongue results in energy peaks, or 'formants' in the spectrum, and it is the location of these formants which differentiates vowel sounds from one another. Thus, the fact that we are able to discriminate 'ae' from 'ih' irrespective of the gender, age or accent of a speaker suggests that we are able to form an invariant representation of the formant relations independently of the fundamental frequency, room reverberations, or spatial location in both quiet and noisy conditions. The aim of this research program is to discover where and how such invariant representations arise in the central auditory system and how these representations are maintained in noisy environments. Forming invariant representations is one of the greatest challenges for sensory systems, and understanding where and how such representations are read out is crucial for the design of any neuroprosthetic device. Our research uses ferrets as their hearing range spans a very similar range of frequencies to ours. Moreover, ferret vocalizations share many similarities with human vowel sounds. Ferrets rapidly learn to discriminate vowel sounds and we are able to record the activity of their nerve cells whilst they perform such listening tasks. By probing the circumstances under which the ferret is able to discriminate vowel sounds, and measuring the neural activity, we can look for where in the auditory brain invariant vowel representation might occur. The second part of this project involves reversibly silencing individual brain areas by cooling them. The principle of this technique is much the same was as using an ice pack to cool pain neurons in a bruised piece of skin. Small 'cryoloops' are implanted above auditory cortex in trained animals.This technique allows us to test whether particular brain areas are causally involved in vowel discrimination. The final part of this project investigates the role of visual information in auditory perception. It is well known that seeing a persons mouth movements while they talk to you enhances your ability to understand them - especially if you are listening in a very noisy room. When trying to pick out a quiet sound in a noisy background knowing when the sound is likely to occur also enhances your ability to correctly identify it. It has recently been shown that visual information is integrated into the very earliest auditory cortical areas. However, quite how this visual information shapes our auditory perception is unknown. The work in this proposal seeks to examine how visual information helps a trained animal to identify vowel sounds more accurately, whilst simultaneously examining how the visual stimulus influences the behaviour of neurons in auditory cortex. Inappropriate integration of auditory and visual information is postulated to underlie schizophrenic symptoms and understanding how informative visual stimuli influence auditory cortical activity will provide valuable insight into how sensory integration occurs in the healthy brain.Hearing impaired individuals most frequently suffer from an inability to effectively identify speech in noisy environments. Understanding how neurons are able to represent vowel identity robustly across a variety of listening conditions and noise environments will enhance hearing aid and cochlear implant design.","grantUrl":"http://gtr.rcuk.ac.uk/projects?ref=BB/H016813/1","grantId":"BB/H016813/1","fundValue":"384541","fundStart":"2011-01-01","fundEnd":"2011-01-01","funder":"BBSRC","impactText":"  Our findings have been presented at a variety of national and international meetings. We have published two review papers and have a paper detailing some behavioural work undergoing a second revision. Our neural findings are currently being prepared for submission for peer review and subsequent publication.\nOur software and methods have been shared with a number of other researchers. In addition the methods that we developed for freely moving recordings have led to me being able to assisted ano Healthcare Societal","person":"Jennifer Kim Bizley","coPersons":[],"organisation":"University of Oxford","findingsText":" We hope that our findings will provide the foundation for work exploring how the auditory brain makes sense of complicated sound scenes - such as listening to conversation in a busy restaurant. Our data provide insight into how the brain achieves this challenge and may in the future provide biologically inspired solutions to computer listening devices or signal processing devices in hearing prosthetics. Continued research Digital/Communication/Information Technologies (including Software),Healthcare","dataset":"gtr"}