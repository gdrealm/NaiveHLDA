{"id":"975E2114-0B9F-44B4-8F82-B7EFE6093C90","title":"SASWAT: Structured Accessibility Stream for Web 2.0 Access Technologies","abstractText":"We are witnessing a profound change in the interaction model of the World Wide Web (Web). Documents, once created from a single source and delivering static client-side content, have now evolved into composite documents created from multiple third party sources delivering dynamically changing information streams. There are few interaction problems when delivering these parallel streams visually. The real problems arise due to the underlying incoherent nature of this 'new' Web model and the composite documents it creates. Changes in context and multiple dynamic updates all compete for the user's attention, producing an incoherent cacophony if the delivery is serial and in audio. Consequently, naive one--shot sensory translation can no longer support the user.This shift in the way the Web works comes with a corresponding increase in the cognitive load required for audio interaction. Without a full understanding of this evolving interaction model, along with its extent and context, the Web will rapidly become unable to support the interaction of visually disabled people.Our objective is to investigate, design, and build a homogeneous mapping framework to support the relating of competing visual streams into a single coherent and mediated accessibility stream such that when automatically applied to a Web document a mapping from parallel visual to serial audio can be achieved. Indeed, because serial mappings are cognitively simpler to understand we would also expect to see side-benefits in cognitive impairment, ageing, and the mobile Web (Whose users share a number of cognitive similarities with visually disabled users -- RIAM EP/E002218/1).To achieve this objective we propose to undertake fundamental research in the areas of: (a) the cognition and perception of dynamic Web based information; (b) the nature of the new Web interaction / infrastructure model as it evolves; and (c) new Web technologies when applied to visually disabled and sighted users. Thus, SASWAT is multidisciplinary with an industrial route to exploitation and has five major aims: 1) Carry out a fundamental investigation of the visual experiences of sighted individuals interacting with competing dynamic information streams in order to better understand the nature of their interaction; 2) Develop a profound understanding of the nature and evolution of the underlying Web infrastructure as it moves from a traditional stateless paradigm to one focused on composite / compound documents and `push' information streams;3) Build a model of Web interaction, based on this investigation, and a mapping of perceptual and cognitive interactivity from sighted to visually disabled users;4) Design and develop an experimental framework to mediate between the competing demands of compound Web pages and multiple information streams;5) Use our corpus of knowledge and experimental tools to perform a systematic and replicable evaluation of the utility of our approaches.","grantUrl":"http://gtr.rcuk.ac.uk/projects?ref=EP/E062954/1","grantId":"EP/E062954/1","fundValue":"351836","fundStart":"2007-07-01","fundEnd":"2010-03-31","funder":"EPSRC","impactText":"  This knowledge enables a better translation form visual to auditory input and removes the threat of the expected auditory cacophony from occurring at anywhere near the levels expected. The work has now received a follow on funding grant from Google who wish to see this work brought to a practical resolution in their 'Chrome' Web Browser. By using UoM technology in partnership with Google technology we hope to to bring real practical benefits to visually disabled Web surfers while contributing back to the research community which initially sponsored this fundamental work. Digital/Communication/Information Technologies (including Software) Economic,Policy & public services","person":"Simon  HARPER","coPersons":[],"organisation":"The University of Manchester","findingsText":" This research work has transformed our understanding of the interaction requirements of visually disabled users. It was originally thought that sighted users would attend to different visual updates in parallel, however, this is not the case. Sighted users prefer to focus on one user initiated task and complete it before moving to the next. This knowledge enables a better translation form visual to auditory input and removes the threat of the expected auditory cacophony from occurring at anywhere near the levels expected. The work has now received a follow on funding grant from Google who wish to see this work brought to a practical resolution in their 'Chrome' Web Browser. By using UoM technology in partnership with Google technology we hope to to bring real practical benefits to visually disabled Web surfers while contributing back to the research community which initially sponsored this fundamental work. We already have uses in the non-academic space via our BBC collaborations. Our original question was 'how can we orchestrate to best effect competing updates occurring over multiple viewports on a single device?' Our new collaborative project wishes to recast this question by asking 'how can we orchestrate to best effect competing updates occurring over a single viewport on multiple devices?' Simply we will apply our models, firstly into the DigitalTV space and then into the dual device presentation space, such that multiple independent devices take part in a single orchestrated presentation. In this way, presentations can be tailored to the needs of the individual on their personal device while still communally interacting on the DigitalTV. The work was primarily focused on blind and mobile users and translated multiple parallel visual updates to a structured serial auditory presentation. In turn, the BBC have become interested in how users experience information presented on dual screens (e.g. a TV and mobile phone or tablet computer) simultaneously – and in particular how synchronised content forms an overall experience across the devices – with a long term aim of developing perceptual and cognitive models of dual screen experience to aid development of future technology. Both organisations see a synergy between these pieces of work and wish to answer the question 'How would we orchestrate to best effect multiple information streams over individual devices, which are all participating in a single unified presentation?' In this case we see a route to knowledge transfer from niche into mainstream technology, while the BBC sees a business case for including our knowledge as a base for their dual screen technology development. Communities and Social Services/Policy,Digital/Communication/Information Technologies (including Software),Transport","dataset":"gtr"}